{"cells":[{"cell_type":"markdown","metadata":{"id":"5rAzoPrxet_n"},"source":["# Costa Rica Institute of Technology\n","* Course: MP-6122 Pattern Recognition\n","* Student: Jose Martinez Hdez\n","* Year: 2022\n","* Laboratory 3: CNN and ANN - MNIST Classification in Keras using a Jetson Nano"]},{"cell_type":"markdown","metadata":{"id":"7eF9y7kXet_s"},"source":["# Introduction\n","\n","According to the [NVIDIA Jetson documentation](https://developer.nvidia.com/embedded/jetson-nano-developer-kit). NVIDIA® Jetson Nano™ Developer Kit is a small, powerful computer that lets you run multiple neural networks in parallel for applications like image classification, object detection, segmentation, and speech processing. All in an easy-to-use platform that runs in as little as 5 watts. The Jetson Nano could be seen in the following image:\n","\n","![jetson](https://drive.google.com/uc?id=12J9vuJEO6vF4UJJVMt2Os0fDUtnb7rFb)\n","\n","This board has the following I/O ports: \n","\n","1. microSD card slot for main storage\n","2. 40-pin expansion header\n","3. Micro-USB port for 5V power input, or for Device Mode\n","4. Gigabit Ethernet port\n","5. USB 3.0 ports (x4)\n","6. HDMI output port\n","7. DisplayPort connector\n","8. DC Barrel jack for 5V power input\n","9. MIPI CSI-2 camera connectors\n","\n","And the technical specifications are:\n","\n","![tech](https://drive.google.com/uc?id=13SMMkuE5Jh0UkOv1TGAbCL97h40hsIN7)\n","\n","**References**: \n","\n","- https://developer.nvidia.com/embedded/jetson-nano-developer-kit\n","- https://developer.nvidia.com/embedded/learn/get-started-jetson-nano-devkit\n","- https://pyimagesearch.com/2019/05/06/getting-started-with-the-nvidia-jetson-nano/"]},{"cell_type":"markdown","metadata":{"id":"EUC_kZ1eet_t"},"source":["# Getting started with the NVIDIA Jetson Nano\n","\n","## Prepare for Setup\n","Before you can even boot up your NVIDIA Jetson Nano you need three things:\n","\n","1. A micro-SD card (minimum 16GB), but it is recommended to use a micro-SD card of 32GB or more.\t\n","2. A 5V 2.5A MicroUSB power supply\n","3. An ethernet cable or Wi-Fi usb adapter (Yes, it doesn't have Wi-Fi embedded on it 😭) \n","\n","To go deeper into the details of the Jetson Nano you can read the following article:\n","\n","- https://developer.nvidia.com/embedded/learn/get-started-jetson-nano-devkit#prepare\n","\n","## Write Image to the microSD Card\n","\n","To prepare your microSD card, you’ll need a computer with Internet connection and the ability to read and write SD cards, either via a built-in SD card slot or adapter.\n","\n","1. Download the [Jetson Nano Developer Kit SD Card Image](https://developer.nvidia.com/jetson-nano-sd-card-image), and note where it was saved on the computer.\n","2. Write the image to your microSD card by following the instructions below according to your computer’s operating system: Windows, macOS, or Linux.\n","\n","    - [Instructions for Windows](https://developer.nvidia.com/embedded/learn/get-started-jetson-nano-devkit#collapse-write_windows)\n","    - [Instructions for MacOS](https://developer.nvidia.com/embedded/learn/get-started-jetson-nano-devkit#collapse-write_macos)\n","    - [Instructions for Linux](https://developer.nvidia.com/embedded/learn/get-started-jetson-nano-devkit#collapse-write_linux)\n","\n","## Setup and First Boot\n","\n","There are two ways to interact with the developer kit: \n","\n","1. With display, keyboard and mouse attached \n","2. In “headless mode” via connection from another computer.\n","\n","You can conduct the initial setup either way. \n","\n","### Initial Setup with Display Attached\n","\n","1. Insert the microSD card (with system image already written to it) into the slot on the underside of the Jetson Nano module.\n","\n","![sd](https://drive.google.com/uc?id=13YjsTnZE2IVrQpAW45zDpcw4ptfrXnor)\n","\n","2. Set the developer kit on top of the paper stand.\n","3. Power on your computer display and connect it.\n","4. Connect the USB keyboard and mouse.\n","4. Connect your Micro-USB power supply (or see the [Jetson Nano Developer Kit User Guide](https://developer.nvidia.com/embedded/learn/get-started-jetson-nano-devkit#) for details about using DC a power supply with a barrel jack connector). The developer kit will power on and boot automatically.\n","\n","In the first boot, a green LED next to the Micro-USB connector will light as soon as the developer kit powers on. When you boot the first time, the developer kit will take you through some initial setup, including:\n","\n","- Review and accept NVIDIA Jetson software EULA\n","- Select system language, keyboard layout, and time zone\n","- Create username, password, and computer name\n","- Select APP partition size—it is recommended to use the max size suggested\n","\n","After logging in, you will see this screen. Congratulations!\n","\n","![screen](https://drive.google.com/uc?id=13YoDJgyMZM2u5BmyvZUAxrYtuvbyiGy2)\n","\n","**References**: \n","\n","- https://developer.nvidia.com/embedded/jetson-nano-developer-kit\n","- https://developer.nvidia.com/embedded/learn/get-started-jetson-nano-devkit\n","- https://pyimagesearch.com/2019/05/06/getting-started-with-the-nvidia-jetson-nano/"]},{"cell_type":"markdown","metadata":{"id":"AKp7YZEzet_v"},"source":["# Running the first code in the Jetson Nano\n","\n","To get started, I really recomended to go over these link provided by NVIDIA:\n","\n","\n","- https://developer.nvidia.com/embedded/community/jetson-projects\n","- https://developer.nvidia.com/embedded/learn/jetson-ai-certification-programs#course_outline\n","- https://github.com/dusty-nv/jetson-inference\n","\n"]},{"cell_type":"markdown","metadata":{"id":"vSj4Y_gqet_w"},"source":["# Example of MNIST model running in the Jetson Nano\n","\n","**Note**: The base code was taken from the following link:\n","\n","- https://keras.io/examples/vision/mnist_convnet/\n","- https://github.com/Tony607/keras_mnist\n","- https://github.com/tflearn/tflearn/blob/master/examples/images/convnet_mnist.py\n","- https://github.com/fmezacr/patrones/tree/master/TfLearn_MNIST_ANN"]},{"cell_type":"markdown","metadata":{"id":"OxDJcGg4et_x"},"source":["## Install dependencies"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24242,"status":"ok","timestamp":1651423041948,"user":{"displayName":"José Martínez","userId":"06561331994642379976"},"user_tz":360},"id":"81dokHHGet_x","outputId":"b15565c7-ce4d-47f1-ff27-2e1e3bc2c6e0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.8.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.4.1)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy) (1.21.6)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.8.0)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.6)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.44.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow) (57.4.0)\n","Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.0.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.14.0)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n","Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.5.3)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (4.2.0)\n","Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n","Collecting tf-estimator-nightly==2.8.0.dev2021122109\n","  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n","\u001b[K     |████████████████████████████████| 462 kB 5.5 MB/s \n","\u001b[?25hRequirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n","Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.25.0)\n","Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (14.0.1)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.35.0)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.23.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.6)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.0.1)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.2.4)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.11.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.8.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2021.10.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n","Installing collected packages: tf-estimator-nightly\n","Successfully installed tf-estimator-nightly-2.8.0.dev2021122109\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (4.1.2.30)\n","Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python) (1.21.6)\n","Collecting tflearn\n","  Downloading tflearn-0.5.0.tar.gz (107 kB)\n","\u001b[K     |████████████████████████████████| 107 kB 5.6 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tflearn) (1.21.6)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tflearn) (1.15.0)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from tflearn) (7.1.2)\n","Building wheels for collected packages: tflearn\n","  Building wheel for tflearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for tflearn: filename=tflearn-0.5.0-py3-none-any.whl size=127299 sha256=8ca0dd6ba11a27370ce2708002157ac5c3ffa1bf3700c0f6c2c10fd4fc4c41e4\n","  Stored in directory: /root/.cache/pip/wheels/5f/14/2e/1d8e28cc47a5a931a2fb82438c9e37ef9246cc6a3774520271\n","Successfully built tflearn\n","Installing collected packages: tflearn\n","Successfully installed tflearn-0.5.0\n"]}],"source":["!pip install keras\n","!pip install scipy\n","!pip install numpy\n","!pip install tensorflow\n","!pip install opencv-python\n","!pip install tflearn"]},{"cell_type":"markdown","metadata":{"id":"T_Dn9Clset_0"},"source":["## Import packages"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":4616,"status":"ok","timestamp":1651423046558,"user":{"displayName":"José Martínez","userId":"06561331994642379976"},"user_tz":360},"id":"IyXzN6IIet_0"},"outputs":[],"source":["from keras import layers, models\n","from keras.datasets import mnist\n","from tensorflow.keras.utils import to_categorical"]},{"cell_type":"markdown","metadata":{"id":"_gNpvgblet_1"},"source":["## Preprocessing"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":809,"status":"ok","timestamp":1651423047347,"user":{"displayName":"José Martínez","userId":"06561331994642379976"},"user_tz":360},"id":"xYka9cUwet_2","outputId":"ff9df242-687f-49e5-87c3-b1ca92fbb3ab"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n","11501568/11490434 [==============================] - 0s 0us/step\n"]}],"source":["# Preprocessing\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","\n","x_train = x_train.astype('float32') / 255\n","x_test = x_test.astype('float32') / 255\n","\n","x_train = x_train.reshape(-1, 28, 28, 1)\n","x_test = x_test.reshape(-1, 28, 28, 1)\n","\n","y_train = to_categorical(y_train)\n","y_test = to_categorical(y_test)"]},{"cell_type":"markdown","metadata":{"id":"zU5BHCDeet_2"},"source":["## EDA"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":299},"executionInfo":{"elapsed":302,"status":"ok","timestamp":1651423047642,"user":{"displayName":"José Martínez","userId":"06561331994642379976"},"user_tz":360},"id":"IvlxOz_wet_2","outputId":"658e2907-205d-4489-d8de-7f890012e148"},"outputs":[{"name":"stdout","output_type":"stream","text":["[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["# Visualizing the data\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# Function for displaying a training image by it's index in the MNIST set\n","def show_digit(index):\n","    label = y_train[index].argmax(axis=0)\n","    # Reshape 784 array into 28x28 image\n","    image = x_train[index].reshape([28,28])\n","    plt.title('Training data, index: %d,  Label: %d' % (index, label))\n","    plt.imshow(image, cmap='gray_r')\n","    plt.show()\n","    \n","# Display the first (index 0) training image\n","print(y_train[0])\n","show_digit(0)"]},{"cell_type":"markdown","metadata":{"id":"eaf5sJXGet_3"},"source":["## Building the network"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4218,"status":"ok","timestamp":1651423051844,"user":{"displayName":"José Martínez","userId":"06561331994642379976"},"user_tz":360},"id":"F3d5m3Jzet_3","outputId":"90a80a39-30b1-4c06-977f-924885eff800"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 26, 26, 16)        160       \n","                                                                 \n"," max_pooling2d (MaxPooling2D  (None, 13, 13, 16)       0         \n"," )                                                               \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 11, 11, 32)        4640      \n","                                                                 \n"," max_pooling2d_1 (MaxPooling  (None, 5, 5, 32)         0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 3, 3, 64)          18496     \n","                                                                 \n"," max_pooling2d_2 (MaxPooling  (None, 1, 1, 64)         0         \n"," 2D)                                                             \n","                                                                 \n"," flatten (Flatten)           (None, 64)                0         \n","                                                                 \n"," dense (Dense)               (None, 256)               16640     \n","                                                                 \n"," dropout (Dropout)           (None, 256)               0         \n","                                                                 \n"," dense_1 (Dense)             (None, 10)                2570      \n","                                                                 \n","=================================================================\n","Total params: 42,506\n","Trainable params: 42,506\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["model = models.Sequential()\n","model.add(layers.Conv2D(16, 3, activation='relu', input_shape=(28, 28, 1)))\n","model.add(layers.MaxPool2D())\n","model.add(layers.Conv2D(32, 3, activation='relu'))\n","model.add(layers.MaxPool2D())\n","model.add(layers.Conv2D(64, 3, activation='relu'))\n","model.add(layers.MaxPool2D())\n","model.add(layers.Flatten())\n","model.add(layers.Dense(256, activation='relu'))\n","model.add(layers.Dropout(0.5))\n","model.add(layers.Dense(10, activation='softmax'))\n","\n","# Print summary of the model\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"qKWhrHVVet_4"},"source":["## Training the network"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":77036,"status":"ok","timestamp":1651423128856,"user":{"displayName":"José Martínez","userId":"06561331994642379976"},"user_tz":360},"id":"v5CBdSzxet_4","outputId":"0c04241b-abdc-4786-98e2-7b6ff06b46e7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/15\n","422/422 [==============================] - 17s 11ms/step - loss: 0.5134 - accuracy: 0.8344 - val_loss: 0.1199 - val_accuracy: 0.9655\n","Epoch 2/15\n","422/422 [==============================] - 4s 10ms/step - loss: 0.1513 - accuracy: 0.9536 - val_loss: 0.0874 - val_accuracy: 0.9757\n","Epoch 3/15\n","422/422 [==============================] - 4s 10ms/step - loss: 0.1111 - accuracy: 0.9669 - val_loss: 0.0772 - val_accuracy: 0.9775\n","Epoch 4/15\n","422/422 [==============================] - 4s 10ms/step - loss: 0.0888 - accuracy: 0.9730 - val_loss: 0.0628 - val_accuracy: 0.9818\n","Epoch 5/15\n","422/422 [==============================] - 4s 10ms/step - loss: 0.0762 - accuracy: 0.9766 - val_loss: 0.0587 - val_accuracy: 0.9827\n","Epoch 6/15\n","422/422 [==============================] - 4s 10ms/step - loss: 0.0662 - accuracy: 0.9795 - val_loss: 0.0484 - val_accuracy: 0.9868\n","Epoch 7/15\n","422/422 [==============================] - 4s 10ms/step - loss: 0.0602 - accuracy: 0.9815 - val_loss: 0.0527 - val_accuracy: 0.9842\n","Epoch 8/15\n","422/422 [==============================] - 4s 10ms/step - loss: 0.0520 - accuracy: 0.9844 - val_loss: 0.0529 - val_accuracy: 0.9848\n","Epoch 9/15\n","422/422 [==============================] - 4s 10ms/step - loss: 0.0471 - accuracy: 0.9857 - val_loss: 0.0450 - val_accuracy: 0.9872\n","Epoch 10/15\n","422/422 [==============================] - 4s 10ms/step - loss: 0.0445 - accuracy: 0.9860 - val_loss: 0.0468 - val_accuracy: 0.9867\n","Epoch 11/15\n","422/422 [==============================] - 4s 10ms/step - loss: 0.0378 - accuracy: 0.9880 - val_loss: 0.0502 - val_accuracy: 0.9868\n","Epoch 12/15\n","422/422 [==============================] - 4s 10ms/step - loss: 0.0378 - accuracy: 0.9876 - val_loss: 0.0473 - val_accuracy: 0.9863\n","Epoch 13/15\n","422/422 [==============================] - 4s 10ms/step - loss: 0.0345 - accuracy: 0.9892 - val_loss: 0.0418 - val_accuracy: 0.9882\n","Epoch 14/15\n","422/422 [==============================] - 4s 10ms/step - loss: 0.0311 - accuracy: 0.9901 - val_loss: 0.0419 - val_accuracy: 0.9875\n","Epoch 15/15\n","422/422 [==============================] - 4s 10ms/step - loss: 0.0282 - accuracy: 0.9913 - val_loss: 0.0460 - val_accuracy: 0.9872\n"]}],"source":["# Train the model\n","model.compile(optimizer='adam', metrics=['accuracy'], loss='categorical_crossentropy')\n","\n","batch_size = 128\n","epochs = 15\n","\n","history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"]},{"cell_type":"markdown","metadata":{"id":"QG-ywU2get_4"},"source":["## Testing the network"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1964,"status":"ok","timestamp":1651423130810,"user":{"displayName":"José Martínez","userId":"06561331994642379976"},"user_tz":360},"id":"iTOoZIjfet_5","outputId":"41e387b1-c654-48a8-824b-deb50ab465c4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Test loss: 0.06291648000478745\n","Test accuracy: 0.982699990272522\n"]}],"source":["# Test the model\n","score = model.evaluate(x_test, y_test, verbose=0)\n","print(\"Test loss:\", score[0])\n","print(\"Test accuracy:\", score[1])"]},{"cell_type":"markdown","metadata":{"id":"SrCSRIciet_5"},"source":["## Save the network"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1651423130811,"user":{"displayName":"José Martínez","userId":"06561331994642379976"},"user_tz":360},"id":"0QbohTEHet_-"},"outputs":[],"source":["# Save model and weights to separated files.\n","with open(\"model.json\", \"w\") as file:\n","    file.write(model.to_json())\n","model.save_weights(\"weights.h5\")\n","\n","# Save model and weights to the same file.\n","model.save('model.h5', include_optimizer=False)\n"]},{"cell_type":"markdown","metadata":{"id":"Q8L1xeylet_-"},"source":["# Using the camera on the Jetson Nano\n","\n","First, let´s install the required dependencies:\n","\n","```bash\n","    sudo apt-get install git cmake\n","    sudo apt-get install libatlas-base-dev gfortran\n","    sudo apt-get install libhdf5-serial-dev hdf5-tools\n","    sudo apt-get install python3-dev\n","```\n","\n","The the python package manager:\n","\n","```bash\n","    wget https://bootstrap.pypa.io/get-pip.py\n","    sudo python3 get-pip.py\n","    rm get-pip.py\n","```\n","\n","We’ll be using Python virtual environments in this guide to keep our Python development environments independent and separate from each other.\n","\n","Using Python virtual environments are a best practice and will help you avoid having to maintain a micro-SD for each development environment you want to use on your Jetson Nano.\n","\n","To manage our Python virtual environments we’ll be using virtualenv and virtualenvwrapper which we can install using the following command:\n","\n","    \n","```bash\n","    sudo pip install virtualenv virtualenvwrapper\n","```\n","\n","Once we’ve installed virtualenv and virtualenvwrapper we need to update our ~/.bashrc\n","\n","```bash\n","    echo \"export WORKON_HOME=$HOME/.virtualenvs\" >> ~/.bashrc\n","    echo \"export VIRTUALENVWRAPPER_PYTHON=/usr/bin/python3\" >> ~/.bashrc\n","    echo \"source /usr/local/bin/virtualenvwrapper.sh\" >> ~/.bashrc\n","```\n","\n","Next, we need to reload the contents of the ~/.bashrc file using the source command:\n","\n","```bash\n","    source ~/.bashrc\n","```\n","\n","Now, we can create our first virtual environment:\n","\n","```bash\n","    mkvirtualenv mnist_env -p /usr/bin/python3\n","    workon mnist_env\n","```\n","\n","At this point, we'll install our application dependencies:\n","\n","```bash\n","    cd mnist\n","    pip install -r requirements.txt\n","```\n","\n","Run the following command to start the camera:\n","\n","``` bash \n","    $ python3 test.py\n","```\n","\n","Note: You might need to change cap = cv2.VideoCapture(0) to the source of the webcam. \n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lp8p22coet__"},"outputs":[],"source":["import cv2\n","\n","cap = cv2.VideoCapture(0)\n","\n","# Check if the webcam is opened correctly\n","if not cap.isOpened():\n","    raise IOError(\"Cannot open webcam\")\n","\n","while True:\n","    ret, frame = cap.read()\n","    frame = cv2.resize(frame, None, fx=0.5, fy=0.5, interpolation=cv2.INTER_AREA)\n","    cv2.imshow('Input', frame)\n","\n","    c = cv2.waitKey(1)\n","    if c == 27:\n","        break\n","\n","cap.release()\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","metadata":{"id":"V-YkGNydet__"},"source":["# Training and running the model on the Jetson Nano\n","\n","Move to the mnist folder:\n","\n","```bash\n","    cd mnist\n","```\n","\n","Then fist we need to train the model:\n","\n","```bash\n","    python3 keras_train.py\n","```\n","\n","Then we need to run the model:\n","\n","```bash\n","    python3 keras_cam.py\n","```"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Laboratory3.ipynb","provenance":[]},"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"},"kernelspec":{"display_name":"Python 3.8.10 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}

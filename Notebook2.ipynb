{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Costa Rica Institute of Technology\n",
    "* Course: MP-6122 Pattern Recognition\n",
    "* Student: Jose Martinez Hdez\n",
    "* Course: Data Science \n",
    "* Year: 2022\n",
    "* Notebook 2: Supervised Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 994,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libaries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 995,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code_number</th>\n",
       "      <th>Clump_Thickness</th>\n",
       "      <th>Uniformity_of_Cell_Size</th>\n",
       "      <th>Uniformity_of_Cell_Shape</th>\n",
       "      <th>Marginal_Adhesion</th>\n",
       "      <th>Single_Epithelial_Cell_Size</th>\n",
       "      <th>Bare_Nuclei</th>\n",
       "      <th>Bland_Chromatin</th>\n",
       "      <th>Normal_Nucleoli</th>\n",
       "      <th>Mitoses</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1002945</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1015425</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1016277</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1017023</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1017122</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   code_number  Clump_Thickness  Uniformity_of_Cell_Size  \\\n",
       "0      1002945                5                        4   \n",
       "1      1015425                3                        1   \n",
       "2      1016277                6                        8   \n",
       "3      1017023                4                        1   \n",
       "4      1017122                8                       10   \n",
       "\n",
       "   Uniformity_of_Cell_Shape  Marginal_Adhesion  Single_Epithelial_Cell_Size  \\\n",
       "0                         4                  5                            7   \n",
       "1                         1                  1                            2   \n",
       "2                         8                  1                            3   \n",
       "3                         1                  3                            2   \n",
       "4                        10                  8                            7   \n",
       "\n",
       "  Bare_Nuclei  Bland_Chromatin  Normal_Nucleoli  Mitoses  Class  \n",
       "0          10                3                2        1      2  \n",
       "1           2                3                1        1      2  \n",
       "2           4                3                7        1      2  \n",
       "3           1                3                1        1      2  \n",
       "4          10                9                7        1      4  "
      ]
     },
     "execution_count": 995,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('dataset/breast-cancer-wisconsin.data')\n",
    "df.columns = ['code_number', 'Clump_Thickness','Uniformity_of_Cell_Size' ,'Uniformity_of_Cell_Shape', 'Marginal_Adhesion',\n",
    "              'Single_Epithelial_Cell_Size',  'Bare_Nuclei', 'Bland_Chromatin', 'Normal_Nucleoli' ,'Mitoses','Class']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 996,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace('?', np.nan)\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 997,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Class = df.Class.map({2: 0, 4:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 998,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 998,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.Class.value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 999,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['code_number','Class'],axis=1).values\n",
    "Y = df.Class.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1000,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 1000,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, random_state=42)\n",
    "df.Class.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1001,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code_number</th>\n",
       "      <th>Clump_Thickness</th>\n",
       "      <th>Uniformity_of_Cell_Size</th>\n",
       "      <th>Uniformity_of_Cell_Shape</th>\n",
       "      <th>Marginal_Adhesion</th>\n",
       "      <th>Single_Epithelial_Cell_Size</th>\n",
       "      <th>Bare_Nuclei</th>\n",
       "      <th>Bland_Chromatin</th>\n",
       "      <th>Normal_Nucleoli</th>\n",
       "      <th>Mitoses</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1002945</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1015425</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1016277</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1017023</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1017122</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   code_number  Clump_Thickness  Uniformity_of_Cell_Size  \\\n",
       "0      1002945                5                        4   \n",
       "1      1015425                3                        1   \n",
       "2      1016277                6                        8   \n",
       "3      1017023                4                        1   \n",
       "4      1017122                8                       10   \n",
       "\n",
       "   Uniformity_of_Cell_Shape  Marginal_Adhesion  Single_Epithelial_Cell_Size  \\\n",
       "0                         4                  5                            7   \n",
       "1                         1                  1                            2   \n",
       "2                         8                  1                            3   \n",
       "3                         1                  3                            2   \n",
       "4                        10                  8                            7   \n",
       "\n",
       "  Bare_Nuclei  Bland_Chromatin  Normal_Nucleoli  Mitoses  Class  \n",
       "0          10                3                2        1      0  \n",
       "1           2                3                1        1      0  \n",
       "2           4                3                7        1      0  \n",
       "3           1                3                1        1      0  \n",
       "4          10                9                7        1      1  "
      ]
     },
     "execution_count": 1001,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "data = pd.read_csv('dataset/housing.csv', header=None, delimiter=r\"\\s+\", names=column_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1002,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_sels = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
    "x = data.loc[:,column_sels]\n",
    "y = data['MEDV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1003,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = MinMaxScaler()\n",
    "x = pd.DataFrame(data=min_max_scaler.fit_transform(x), columns=column_sels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression (Regression)\n",
    "\n",
    "### Conceptual Explanation \n",
    "\n",
    "Linear Regression is the supervised machine learning model in which the model finds the best fit linear line between the independent and dependent variable i.e it finds the linear relationship between the dependent and independent variable.\n",
    "\n",
    "<img src=\"images/linear.png\" width=400 height=400 />\n",
    "\n",
    "### Mathematical \n",
    "\n",
    "From basic linear algebra, we should remember that the linear regression model is a linear combination of the independent variables, that is described by the following equation: \n",
    "\n",
    "\\begin{equation}\n",
    "y = b_0 + b_1 \\cdot x\n",
    "\\end{equation}\n",
    "\n",
    "Where $b_0$ is the intercept and $b_1$ is the slope.\n",
    "\n",
    "So, the main idea of linear regression is to find the best fit line between the independent and dependent variables. This can be done by finding the value of the slope ($b_1$) and the intercept ($b_0$) in the linear equation that reduces the error between a predicted value from the model and the actual value from the data. \n",
    "\n",
    "We create a hipotetic model by using the following equation:\n",
    "\n",
    "\\begin{equation}\n",
    "h(x) = b_0 + b_1 \\cdot x\n",
    "\\end{equation}\n",
    "\n",
    "And then we calculate the error between the actual value and the predicted value by using the following cost equation of the squared error:\n",
    "\n",
    "\\begin{equation}\n",
    "J(b_0, b_1) = \\frac{1}{2m} \\sum_{1=n}^{i=1} (h(x_n) - y_n)^2\n",
    "\\end{equation}\n",
    "\n",
    "### Optimal Applications \n",
    "\n",
    "This model has the following assumptions and limitations: \n",
    "\n",
    "1. *Linearity*: The relationship between the dependent and independent variables is linear.\n",
    "2. *Homoscedasticity*: The error is homoscedastic. The variance of the error terms should be constant i.e the spread of residuals should be constant for all values of X. This assumption can be checked by plotting a residual plot. If the assumption is violated then the points will form a funnel shape otherwise they will be constant.\n",
    "3. *Independence*:  The variables should be independent of each other i.e no correlation should be there between the independent variables.\n",
    "4. *Normality*: The $x$ and $y$ variables should be normally distributed.\n",
    "\n",
    "Basically, this model will work well when the assumptions are satisfied. The violation of the assumptions leads to a decrease in the accuracy of the model therefore the predictions are not accurate and error is also high.\n",
    "\n",
    "**References**: \n",
    "\n",
    "- https://towardsdatascience.com/ensemble-models-5a62d4f4cb0chttps://www.analyticsvidhya.com/blog/2021/05/all-you-need-to-know-about-your-first-machine-learning-model-linear-regression/#:~:text=In%20the%20most%20simple%20words,the%20dependent%20and%20independent%20variable.\n",
    "- https://www.youtube.com/watch?v=1-OGRohmH2s\n",
    "- https://towardsdatascience.com/mathematics-for-machine-learning-linear-regression-least-square-regression-de09cf53757c\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "- https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1004,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7406426641094095\n",
      "[ -9.60975755   4.64204584   0.56083933   2.68673382  -8.63457306\n",
      "  19.88368651   0.06721501 -16.22666104   7.03913802  -6.46332721\n",
      "  -8.95582398   3.69282735 -19.01724361]\n",
      "26.620267584687756\n"
     ]
    }
   ],
   "source": [
    "# Example of Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(x, y)\n",
    "print(regressor.score(x, y))\n",
    "print(regressor.coef_)\n",
    "print(regressor.intercept_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression (Classification)\n",
    "\n",
    "### Conceptual Explanation \n",
    "\n",
    "Logistic regression, despite its name, is a classification model rather than regression model. Logistic regression is a simple and more efficient method for binary and linear classification\n",
    "problems. It is a classification model, which is very easy to realize and achieves very good performance with linearly separable classes. It belongs to the group of linear classifiers and is somewhat similar to polynomial and linear regression. Logistic regression is fast and relatively uncomplicated, and it’s convenient for you to interpret the results. Although it’s essentially a method for binary classification, it can also be applied to multiclass problems.\n",
    "\n",
    "![sigmoid](images/sigmoid.png)\n",
    "\n",
    "### Mathematical \n",
    "\n",
    "The logistic regression model is a linear combination of the independent variables, that is described by the following equation:\n",
    "\n",
    "\\begin{equation}\n",
    "f(x) = \\frac{1}{1 + e^{-m \\cdot x + b}}\n",
    "\\end{equation}\n",
    "\n",
    "That function is also called the sigmoid function. A sigmoid function is a mathematical function having a characteristic \"S\"-shaped curve or sigmoid curve. The sigmoid function has values very close to either 0 or 1 across most of its domain. This fact makes it suitable for application in classification methods.\n",
    "\n",
    "In this model, we have the linear combination of the independent variables, that is described by the following equation:\n",
    "\n",
    "\\begin{equation}\n",
    "h(x) = b_0 + b_1 \\cdot x_1 + b_2 \\cdot x_2 + ... + b_n \\cdot x_n\n",
    "\\end{equation}\n",
    "\n",
    "That función is called the *logit*. The variables $b_0, b_1, b_2, ..., b_n$ are the coefficients or preticted weights of the linear combination that comes from the $-m \\cdot x + b$ term of sigmoid function. Similar as in linear regression, the idea is to create a cost function that determines the best predicted weights $b_0, b_1, b_2, ..., b_n$  such that the function $f(x)$ is as close as possible to all actual responses $𝑦_i$ values. \n",
    "\n",
    "\\begin{equation}\n",
    "J(b_0, b_1, .., b_n) = \\sum_{1=n}^{i=1} y_n\\log(f(x_n)) + (1-y_n)\\log(1-f(x_n))\n",
    "\\end{equation}\n",
    "\n",
    "The idea of using logarithms is to simplify the cost function. By minimizing the cost function, we can find the best fit line between the independent and dependent variables. The most common way to solve the logistic regression problem is to use the gradient descent algorithm and also with the maximum likelihood estimator (MLE). The main aim of MLE is to find the value of our parameters for which the likelihood function is maximized. The likelihood function is nothing but a joint pdf of our sample observations and joint distribution is the multiplication of the conditional probability for observing each example given the distribution parameters. \n",
    "\n",
    "Gradient descent changes the value of our weights in such a way that it always converges to minimum point or we can also say that, it aims at finding the optimal weights which minimize the loss function of our model. It is an iterative method that finds the minimum of a function by figuring out the slope at a random point and then moving in the opposite direction.\n",
    "\n",
    "### Optimal Applications\n",
    "\n",
    "Logistic regresssion performs well for binary classification problems. It is also a good model for classification problems where the dependent variable is categorical. It has the advantage of being fast and easy to interpret.\n",
    "\n",
    "Lastly, the most significant advantage of logistic regression over other algoritmhs is transparency. Complex models, as neural networks, work as a black box - you never know why it makes one or another decision. There are a lot of highly regulated industries where this approach is not acceptable. Logistic regression, in contrast, may be called the “white box”. You always know why you rejected a loan application or why your patient’s diagnosis looks good or bad. That is what we’ll talk about in detail.\n",
    "\n",
    "**References**: \n",
    "\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "- Practical Machine Learning for Data Analysis Using Python\n",
    "- Machine Learning Guide for Oil and Gas Using Python\n",
    "- https://realpython.com/logistic-regression-python/#logistic-regression-overview\n",
    "- https://en.wikipedia.org/wiki/Sigmoid_function\n",
    "- https://www.analyticsvidhya.com/blog/2021/08/conceptual-understanding-of-logistic-regression-for-data-science-beginners/#:~:text=Logistic%20Regression%20is%20another%20statistical,pass%20this%20exam%20or%20not\n",
    "- https://medium.com/analytics-vidhya/logistic-regression-b35d2801a29c\n",
    "- https://activewizards.com/blog/5-real-world-examples-of-logistic-regression-application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1005,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score is 0.9562043795620438\n",
      "[0 1]\n",
      "[[0.37455966 0.21822076 0.50280676 0.2547255  0.02534985 0.34896106\n",
      "  0.29500212 0.23440608 0.46061253]]\n",
      "[-9.5864799]\n"
     ]
    }
   ],
   "source": [
    "# Example of Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "y_hat = clf.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "print(f'Accuracy Score is {accuracy_score(y_test, y_hat)}')\n",
    "\n",
    "print(clf.classes_)\n",
    "print(clf.coef_)\n",
    "print(clf.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines (Classification)\n",
    "\n",
    "### Conceptual Explanation \n",
    "\n",
    "Support Vector Machine (SVM) is a supervised machine learning algorithm that can be used for both classification or regression challenges. However,  it is mostly used in classification problems. In the SVM algorithm, we plot each data item as a point in n-dimensional space (where n is a number of features you have) with the value of each feature being the value of a particular coordinate. Then, we perform classification by finding the hyper-plane that differentiates the two classes very well, as it could be seen in the following figure.\n",
    "\n",
    "![svm](images/svm.png)\n",
    "\n",
    "### Mathematical \n",
    "\n",
    "The main objective of SVM is to find the optimal hyperplane which linearly separates the data points in two component by maximizing the margin.\n",
    "\n",
    "A hyper-plane means plane that linearly divide the n-dimensional data points in two component. In case of 2D, hyperplane is line, in case of 3D it is plane.It is also called as n-dimensional line. That hyper-plane is defined by the following equation, for the case of 2D:\n",
    "\n",
    "\\begin{equation}\n",
    "H(x): w\\cdot x + b=0\n",
    "\\end{equation}\n",
    "\n",
    "SVM problem can be formulated as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{matrix}\n",
    "w \\cdot x_i + b \\geq  1 & y_i = +1 \\\\\n",
    "w \\cdot x_i + b \\leq  -1 & y_i = -1 \\\\\n",
    "\\end{matrix}\n",
    "\\end{equation}\n",
    "\n",
    "Combined with the above equation, we can rewrite the equation as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{matrix}\n",
    "y_i (w \\cdot x_i + b) - 1 \\geq 0 & y_i = +1, -1 \\\\\n",
    "\\end{matrix}\n",
    "\\end{equation}\n",
    "\n",
    "The distance between a point and a line can be calculated as:\n",
    "\n",
    "\\begin{equation}\n",
    "d =\\frac{\\left| a \\cdot x_0 + b \\cdot y_0 +c \\right|}{\\sqrt{a^2 + b^2}}\n",
    "\\end{equation}\n",
    "\n",
    "For a hyper-plane, the distance between a point and a line can be calculated as:\n",
    "\n",
    "\\begin{equation}\n",
    "d_H =\\frac{\\left| w(x_0) + b \\right|}{\\left|\\left| w \\right| \\right|}\n",
    "\\end{equation}\n",
    "\n",
    "Since the idea is to maximize the distance, we should minimize the euclidean distance. So, we can rewrite the equation as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\left|\\left| w \\right| \\right| = \\sqrt{\\sum_{j=1}^{D} w_j^2}\n",
    "\\end{equation}\n",
    "\n",
    "Finally, the optimization criterion is:\n",
    "\n",
    "\\begin{equation}\n",
    "min \\left|\\left| w \\right| \\right| = min \\frac{1}{2} \\left|\\left| w \\right| \\right|^2 \n",
    "\\end{equation}\n",
    "\n",
    "What if data points is not linearly separable?\n",
    "\n",
    "In such cases, we do not see a straight line frontier directly in current plane which can serve as the SVM. In such cases, we need to map these vector to a higher dimension plane so that they get segregated from each other, that algorithm is called kernel SVM, which well be covered in the next sections.\n",
    "\n",
    "### Optimal Applications\n",
    "\n",
    "Some of the pros of SVM are:\n",
    "\n",
    "1. It works really well with a clear margin of separation.\n",
    "2. It is effective in high dimensional spaces.\n",
    "3. It is effective in cases where the number of dimensions is greater than the number of samples.\n",
    "4. It uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
    "\n",
    "In the other hand, some of the cons of SVM are:\n",
    "\n",
    "1. It doesn’t perform well when we have large data set because the required training time is higher.\n",
    "2. It also doesn’t perform very well, when the data set has more noise i.e. target classes are overlapping.\n",
    "3. SVM doesn’t directly provide probability estimates, these are calculated using an expensive five-fold cross-validation. \n",
    "\n",
    "**References**: \n",
    "\n",
    "- https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47\n",
    "- https://scikit-learn.org/stable/modules/classes.html#module-sklearn.svm\n",
    "- https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/\n",
    "- https://www.analyticsvidhya.com/blog/2014/10/support-vector-machine-simplified/?utm_source=blog&utm_medium=understandingsupportvectormachinearticle\n",
    "- https://www.analyticsvidhya.com/blog/2020/10/the-mathematics-behind-svm/#:~:text=A%20Support%20Vector%20Machine%20or,to%20as%20Support%20Vector%20Classification.\n",
    "- https://ankitnitjsr13.medium.com/math-behind-support-vector-machine-svm-5e7376d0ee4d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1006,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score is 0.9635036496350365\n"
     ]
    }
   ],
   "source": [
    "# Example of SVM\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(X_train, y_train)\n",
    "y_hat = clf.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "print(f'Accuracy Score is {accuracy_score(y_test, y_hat)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Models for multi-class classification (Classification)\n",
    "\n",
    "### Conceptual Explanation \n",
    "\n",
    "![multi-class](images/multi.png)\n",
    "\n",
    "### Mathematical \n",
    "\n",
    "\\begin{equation}\n",
    "y = b_0 + b_1 \\cdot x_1 + b_2 \\cdot x_2 + ... + b_n \\cdot x_n\n",
    "\\end{equation}\n",
    "\n",
    "### Optimal Applications\n",
    "\n",
    "**References**: \n",
    "\n",
    "- https://scikit-learn.org/stable/modules/classes.html#module-sklearn.multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1007,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score is 0.9635036496350365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jpmh1\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    }
   ],
   "source": [
    "# Example of Linear Models for multi-class classification\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "clf = OneVsRestClassifier(LinearSVC(random_state=0))\n",
    "clf.fit(X_train, y_train)\n",
    "y_hat = clf.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "print(f'Accuracy Score is {accuracy_score(y_test, y_hat)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN (Classification or Regression)) \n",
    "\n",
    "### Conceptual Explanation \n",
    "\n",
    "Linear Regression is the supervised Machine Learning model in which the model finds the best fit linear line between the independent and dependent variable i.e it finds the linear relationship between the dependent and independent variable.\n",
    "\n",
    "![knn](images/knn.png)\n",
    "\n",
    "### Mathematical \n",
    "\n",
    "\\begin{equation}\n",
    "y = b_0 + b_1 \\cdot x\n",
    "\\end{equation}\n",
    "\n",
    "### Optimal Applications\n",
    "\n",
    "\n",
    "**References**: \n",
    "\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1008,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score is 0.9562043795620438\n"
     ]
    }
   ],
   "source": [
    "# Example of KNN Classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=3)\n",
    "clf.fit(X_train, y_train)\n",
    "y_hat = clf.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "print(f'Accuracy Score is {accuracy_score(y_test, y_hat)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1009,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.850617542134604\n"
     ]
    }
   ],
   "source": [
    "# Example of kNN Regression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "regressor = KNeighborsRegressor()\n",
    "regressor.fit(x, y)\n",
    "print(regressor.score(x, y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes (Classification)\n",
    "\n",
    "### Conceptual Explanation \n",
    "\n",
    "In statistics, naive Bayes classifiers are a family of simple \"probabilistic classifiers\" based on applying Bayes' theorem with strong (naive) independence assumptions between the features (see Bayes classifier). They are among the simplest Bayesian network models, but coupled with kernel density estimation, they can achieve high accuracy levels.\n",
    "\n",
    "![naive](images/naive.png)\n",
    "\n",
    "### Mathematical \n",
    "\n",
    "\\begin{equation}\n",
    "y = b_0 + b_1 \\cdot x\n",
    "\\end{equation}\n",
    "\n",
    "### Optimal Applications\n",
    "\n",
    "**References**: \n",
    "\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1010,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score is 0.8978102189781022\n"
     ]
    }
   ],
   "source": [
    "# Example of Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB(alpha=1.0)\n",
    "clf.fit(X_train, y_train)\n",
    "y_hat = clf.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "print(f'Accuracy Score is {accuracy_score(y_test, y_hat)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees (Classification or Regression)) \n",
    "\n",
    "### Conceptual Explanation \n",
    "\n",
    "Linear Regression is the supervised Machine Learning model in which the model finds the best fit linear line between the independent and dependent variable i.e it finds the linear relationship between the dependent and independent variable.\n",
    "\n",
    "![decision-tree](images/decision-tree.png)\n",
    "\n",
    "### Mathematical \n",
    "\n",
    "$y = b_0 + b_1 \\cdot x$\n",
    "\n",
    "### Optimal Applications\n",
    "\n",
    "**References**: \n",
    "\n",
    "- https://www.datacamp.com/community/tutorials/decision-tree-classification-python\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1011,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score is 0.9416058394160584\n"
     ]
    }
   ],
   "source": [
    "# Example of Decision Trees Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=2, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "y_hat = clf.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "print(f'Accuracy Score is {accuracy_score(y_test, y_hat)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1012,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Example of Decision Trees Regression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "regressor = DecisionTreeRegressor(random_state=0)\n",
    "regressor.fit(x, y)\n",
    "print(regressor.score(x, y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests (Classification or Regression)\n",
    "\n",
    "### Conceptual Explanation \n",
    "\n",
    "Linear Regression is the supervised Machine Learning model in which the model finds the best fit linear line between the independent and dependent variable i.e it finds the linear relationship between the dependent and independent variable.\n",
    "\n",
    "![random-forest](images/random-forest.svg)\n",
    "\n",
    "### Mathematical \n",
    "\n",
    "\\begin{equation}\n",
    "y = b_0 + b_1 \\cdot x\n",
    "\\end{equation}\n",
    "\n",
    "### Optimal Applications\n",
    "\n",
    "**References**: \n",
    "\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1013,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score is 0.9562043795620438\n"
     ]
    }
   ],
   "source": [
    "# Example of Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "y_hat = clf.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "print(f'Accuracy Score is {accuracy_score(y_test, y_hat)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1014,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.763042092638991\n"
     ]
    }
   ],
   "source": [
    "# Example of Random Forest Regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "regressor = RandomForestRegressor(max_depth=2, random_state=0)\n",
    "regressor.fit(x, y)\n",
    "print(regressor.score(x, y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Support Vector Machines (Classification)\n",
    "\n",
    "### Conceptual Explanation \n",
    "\n",
    "Linear Regression is the supervised Machine Learning model in which the model finds the best fit linear line between the independent and dependent variable i.e it finds the linear relationship between the dependent and independent variable.\n",
    "\n",
    "![kernel](images/kernel.png)\n",
    "\n",
    "### Mathematical \n",
    "\n",
    "\\begin{equation}\n",
    "y = b_0 + b_1 \\cdot x\n",
    "\\end{equation}\n",
    "\n",
    "### Optimal Applications\n",
    "\n",
    "**References**: \n",
    "\n",
    "- *link*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1015,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score is 0.9635036496350365\n"
     ]
    }
   ],
   "source": [
    "# Example of kernel SVM\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC(kernel='rbf', C=10, gamma=0.1)\n",
    "clf.fit(X_train, y_train)\n",
    "y_hat = clf.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "print(f'Accuracy Score is {accuracy_score(y_test, y_hat)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods\n",
    "\n",
    "**References**: \n",
    "\n",
    "- https://towardsdatascience.com/ensemble-models-5a62d4f4cb0c\n",
    "- https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Ensemble Methods\n",
    "\n",
    "## Max Voting\n",
    "\n",
    "### Conceptual Explanation \n",
    "\n",
    "Linear Regression is the supervised Machine Learning model in which the model finds the best fit linear line between the independent and dependent variable i.e it finds the linear relationship between the dependent and independent variable.\n",
    "\n",
    "### Mathematical \n",
    "\n",
    "\\begin{equation}\n",
    "y = b_0 + b_1 \\cdot x\n",
    "\\end{equation}\n",
    "\n",
    "### Optimal Applications\n",
    "\n",
    "### Example of the model\n",
    "\n",
    "**References**: \n",
    "\n",
    "- *link* \n",
    "\n",
    "## Averaging\n",
    "\n",
    "### Conceptual Explanation \n",
    "\n",
    "Linear Regression is the supervised Machine Learning model in which the model finds the best fit linear line between the independent and dependent variable i.e it finds the linear relationship between the dependent and independent variable.\n",
    "\n",
    "### Mathematical \n",
    "\n",
    "\\begin{equation}\n",
    "y = b_0 + b_1 \\cdot x\n",
    "\\end{equation}\n",
    "\n",
    "### Optimal Applications\n",
    "\n",
    "### Example of the model\n",
    "\n",
    "**References**: \n",
    "\n",
    "- *link* \n",
    "\n",
    "## Weighted Average\n",
    "\n",
    "### Conceptual Explanation \n",
    "\n",
    "Linear Regression is the supervised Machine Learning model in which the model finds the best fit linear line between the independent and dependent variable i.e it finds the linear relationship between the dependent and independent variable.\n",
    "\n",
    "### Mathematical \n",
    "\n",
    "\\begin{equation}\n",
    "y = b_0 + b_1 \\cdot x\n",
    "\\end{equation}\n",
    "\n",
    "### Optimal Applications\n",
    "\n",
    "### Example of the model\n",
    "\n",
    "**References**: \n",
    "\n",
    "- *link* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1016,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of simple emsemble methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Ensemble Methods\n",
    "\n",
    "## Bagging\n",
    "\n",
    "### Conceptual Explanation \n",
    "\n",
    "Linear Regression is the supervised Machine Learning model in which the model finds the best fit linear line between the independent and dependent variable i.e it finds the linear relationship between the dependent and independent variable.\n",
    "\n",
    "### Mathematical \n",
    "\n",
    "\\begin{equation}\n",
    "y = b_0 + b_1 \\cdot x\n",
    "\\end{equation}\n",
    "\n",
    "### Optimal Applications\n",
    "\n",
    "### Example of the model\n",
    "\n",
    "**References**: \n",
    "\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html#sklearn.ensemble.BaggingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1017,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score is 0.9708029197080292\n"
     ]
    }
   ],
   "source": [
    "# Example of Bagging Classifier \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "clf = BaggingClassifier(base_estimator=SVC(), n_estimators=10, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "y_hat = clf.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "print(f'Accuracy Score is {accuracy_score(y_test, y_hat)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "### Conceptual Explanation \n",
    "\n",
    "Linear Regression is the supervised Machine Learning model in which the model finds the best fit linear line between the independent and dependent variable i.e it finds the linear relationship between the dependent and independent variable.\n",
    "\n",
    "### Mathematical \n",
    "\n",
    "\\begin{equation}\n",
    "y = b_0 + b_1 \\cdot x\n",
    "\\end{equation}\n",
    "\n",
    "### Optimal Applications\n",
    "\n",
    "### Example of the model\n",
    "\n",
    "**References**: \n",
    "\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1018,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score is 0.9708029197080292\n"
     ]
    }
   ],
   "source": [
    "# Example of Bosting Classifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "y_hat = clf.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "print(f'Accuracy Score is {accuracy_score(y_test, y_hat)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking\n",
    "\n",
    "### Conceptual Explanation \n",
    "\n",
    "Linear Regression is the supervised Machine Learning model in which the model finds the best fit linear line between the independent and dependent variable i.e it finds the linear relationship between the dependent and independent variable.\n",
    "\n",
    "### Mathematical \n",
    "\n",
    "\\begin{equation}\n",
    "y = b_0 + b_1 \\cdot x\n",
    "\\end{equation}\n",
    "\n",
    "### Optimal Applications\n",
    "\n",
    "### Example of the model\n",
    "\n",
    "**References**: \n",
    "\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1019,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score is 0.9562043795620438\n"
     ]
    }
   ],
   "source": [
    "# Example of Stacking Classifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "estimators = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=10, random_state=42)),\n",
    "    ('svr', make_pipeline(StandardScaler(), LinearSVC(random_state=42)))\n",
    "]\n",
    "\n",
    "clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n",
    "clf.fit(X_train, y_train)\n",
    "y_hat = clf.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "print(f'Accuracy Score is {accuracy_score(y_test, y_hat)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blending\n",
    "\n",
    "### Conceptual Explanation \n",
    "\n",
    "Linear Regression is the supervised Machine Learning model in which the model finds the best fit linear line between the independent and dependent variable i.e it finds the linear relationship between the dependent and independent variable.\n",
    "\n",
    "### Mathematical \n",
    "\n",
    "\\begin{equation}\n",
    "y = b_0 + b_1 \\cdot x\n",
    "\\end{equation}\n",
    "\n",
    "### Optimal Applications\n",
    "\n",
    "### Example of the model\n",
    "\n",
    "**References**: \n",
    "\n",
    "- *link* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1020,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score is 0.9708029197080292\n"
     ]
    }
   ],
   "source": [
    "# Example of Blending Classifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "y_hat = clf.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "print(f'Accuracy Score is {accuracy_score(y_test, y_hat)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is it possible to create our own ensemble model? If so, how would it be the approach?\n",
    "\n",
    "**References**: \n",
    "\n",
    "- *link* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "\n",
    "Performance metrics are a part of every machine learning pipeline. They tell you if you’re making progress, and put a number on it. All machine learning models, whether it’s linear regression, or a SOTA technique like BERT, need a metric to judge performance\n",
    "\n",
    "**References**: \n",
    "\n",
    "- https://neptune.ai/blog/performance-metrics-in-machine-learning-complete-guide\n",
    "- https://scikit-learn.org/stable/modules/model_evaluation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Metrics\n",
    "\n",
    "Classification models have discrete output, so we need a metric that compares discrete classes in some form. Classification Metrics evaluate a model’s performance and tell you how good or bad the classification is, but each of them evaluates it in a different way.\n",
    "\n",
    "The following are the metrics for evaluating classification problems:\n",
    "\n",
    "- Accuracy\n",
    "- Balanced Accuracy\n",
    "- Top k Accuracy\n",
    "- Average Precision\n",
    "- Brier Score\n",
    "- F1 Score\n",
    "- F1 Micro Score\n",
    "- F1 Macro Score\n",
    "- F1 Weighted Score\n",
    "- F1 Sample Score\n",
    "- Log Loss\n",
    "- Precision\n",
    "- Recall\n",
    "- Jaccard\n",
    "- Roc Auc\n",
    "- Roc Auc Overlap\n",
    "- Roc Auc Under Over\n",
    "\n",
    "In the following seccion, we will discuss more details of the most common metrics.\n",
    "\n",
    "### Confusion Matrix\n",
    "\n",
    "Confusion Matrix is a tabular visualization of the ground-truth labels versus model predictions. Each row of the confusion matrix represents the instances in a predicted class and each column represents the instances in an actual class. Confusion Matrix is not exactly a performance metric but sort of a basis on which other metrics evaluate the results. One example of a confusion matrix is the following image:\n",
    "\n",
    "![confusion](images/confusion.png)\n",
    "\n",
    "The confusion matrix is made up of the following elements:\n",
    "\n",
    "- **True Positives**: Number of correct predictions of the positive class.\n",
    "- **True Negatives**: Number of correct predictions of the negative class.\n",
    "- **False Positives**: Number of incorrect predictions of the positive class.\n",
    "- **False Negatives**: Number of incorrect predictions of the negative class.\n",
    "\n",
    "The idea is to try minimize the **False Positives** and **False Negatives**, since these are the ones that are most likely to be misclassified.\n",
    "\n",
    "### Accuracy\n",
    "\n",
    "Classification accuracy is perhaps the simplest metric to use and implement and is defined as the number of correct predictions divided by the total number of predictions, multiplied by 100.\n",
    "\n",
    "This metric is described with the following equation:\n",
    "\n",
    "\\begin{equation}\n",
    "Accuracy = \\frac{TP + TN}{TP + FP + TN + FN}\n",
    "\\end{equation}\n",
    "\n",
    "### Precision\n",
    "\n",
    "There are many cases in which classification accuracy is not a good indicator of your model performance. One of these scenarios is when your class distribution is imbalanced (one class is more frequent than others). In this case, even if you predict all samples as the most frequent class you would get a high accuracy rate, which does not make sense at all (because your model is not learning anything, and is just predicting everything as the top class). Precision is the ratio of true positives and total positives predicted. \n",
    "\n",
    "This metric is described with the following equation:\n",
    "\n",
    "\\begin{equation}\n",
    "Precision = \\frac{TP}{TP+FP}\n",
    "\\end{equation}\n",
    "\n",
    "### Recall\n",
    "\n",
    "Recall is another important metric, which is defined as the fraction of samples from a class which are correctly predicted by the model.\n",
    "\n",
    "This metric is described with the following equation:\n",
    "\n",
    "\\begin{equation}\n",
    "Recall = \\frac{TP}{TP+FN}\n",
    "\\end{equation}\n",
    "\n",
    "### F1\n",
    "\n",
    "Depending on application, you may want to give higher priority to recall or precision. But there are many applications in which both recall and precision are important. Therefore, it is natural to think of a way to combine these two into a single metric. One popular metric which combines precision and recall is called F1-score, which is the harmonic mean of precision and recall.\n",
    "\n",
    "This metric is described with the following equation:\n",
    "\n",
    "\\begin{equation}\n",
    "F1 = \\frac{1}{\\frac{1}{Precision} + \\frac{1}{Recall}}\n",
    "\\end{equation}\n",
    "\n",
    "### Receiver Operating Curve (ROC) \n",
    "\n",
    "The receiver operating characteristic curve is plot which shows the performance of a binary classifier as function of its cut-off threshold. It essentially shows the true positive rate (TPR) against the false positive rate (FPR) for various threshold values.\n",
    "\n",
    "One example of a ROC curve is the following image:\n",
    "\n",
    "![roc](images/roc.png)\n",
    "\n",
    "### Area Under the Curve (AUC)\n",
    "\n",
    "The area under the curve (AUC), is an aggregated measure of performance of a binary classifier on all possible threshold values (and therefore it is threshold invariant). AUC calculates the area under the ROC curve, and therefore it is between 0 and 1. One way of interpreting AUC is as the probability that the model ranks a random positive example more highly than a random negative example.\n",
    "\n",
    "One example of a AUC curve is the following image:\n",
    "\n",
    "![auc](images/auc.png)\n",
    "\n",
    "**References**: \n",
    "\n",
    "- https://neptune.ai/blog/performance-metrics-in-machine-learning-complete-guide#:~:text=Classification%20models%20have%20discrete%20output,it%20in%20a%20different%20way.\n",
    "- https://medium.com/@MohammedS/performance-metrics-for-classification-problems-in-machine-learning-part-i-b085d432082b\n",
    "- https://towardsdatascience.com/20-popular-machine-learning-metrics-part-1-classification-regression-evaluation-metrics-1ca3e282a2ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1021,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Example of Classification Metrics\n",
    "# Notebook from: https://app.neptune.ai/theaayushbajaj/sandbox/n/f884bbea-5263-4aeb-aa35-18d74b2835b9/41813125-2b9d-4332-b73f-f07c3b977372\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Train a model\n",
    "clf = SVC(\n",
    "        C=1.0,\n",
    "        kernel='rbf',\n",
    "        degree=3,\n",
    "        gamma='scale',\n",
    "        coef0=0.0,\n",
    "        shrinking=True,\n",
    "        probability=False,\n",
    "        tol=0.001,\n",
    "        cache_size=200,\n",
    "        class_weight=None,\n",
    "        verbose=False,\n",
    "        max_iter=-1,\n",
    "        decision_function_shape='ovr',\n",
    "        break_ties=False,\n",
    "        random_state=None\n",
    "    )\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "y_hat = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1022,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[85  2]\n",
      " [ 3 47]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_hat)\n",
    "print('Confusion Matrix:')\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1023,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score is 0.9635036496350365\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Accuracy\n",
    "print(f'Accuracy Score is {accuracy_score(y_test,y_hat)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1024,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.959184\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "# Precision\n",
    "precision = precision_score(y_test, y_hat)\n",
    "print('Precision: %f' % precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1025,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.940000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "# Recall\n",
    "recall = recall_score(y_test, y_hat)\n",
    "print('Recall: %f' % recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1026,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.949495\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "# F1 Score\n",
    "f1 = f1_score(y_test, y_hat)\n",
    "print('F1 Score: %f' % f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1027,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, '2-class Precision-Recall curve: AP=0.96')"
      ]
     },
     "execution_count": 1027,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "\n",
    "# Plot Precision-Recall Curve\n",
    "disp = plot_precision_recall_curve(clf, X, Y)\n",
    "disp.ax_.set_title('2-class Precision-Recall curve: AP={0:0.2f}'.format(precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1028,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Skill: ROC AUC=0.500\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "ns_probs = [0 for _ in range(len(Y))]\n",
    "\n",
    "# calculate scores\n",
    "ns_auc = roc_auc_score(Y, ns_probs)\n",
    "\n",
    "# summarize scores\n",
    "print('No Skill: ROC AUC=%.3f' % (ns_auc))\n",
    "\n",
    "# calculate roc curves\n",
    "ns_fpr, ns_tpr, _ = roc_curve(Y, ns_probs)\n",
    "\n",
    "# plot the roc curve for the model\n",
    "pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Metrics\n",
    "\n",
    "Regression refers to predictive modeling problems that involve predicting a numeric value.\n",
    "\n",
    "It is different from classification that involves predicting a class label. Unlike classification, you cannot use classification accuracy to evaluate the predictions made by a regression model. Instead, you must use error metrics specifically designed for evaluating predictions made on regression problems and it is most Robust to outliers.\n",
    "\n",
    "The following are the metrics for evaluating regression problems:\n",
    "\n",
    "- Max Error\n",
    "- Mean Absolute Error\n",
    "- Mean Squared Error\n",
    "- Root Mean Squared Error\n",
    "- Mean Squared Logarithmic Error\n",
    "- Median Absolute Error\n",
    "- R-Squared\n",
    "- Explained Variance\n",
    "- Mean Poisson Deviance\n",
    "- Mean Gamma Deviance\n",
    "- Mean Absolute Percentage Error\n",
    "\n",
    "In the following seccion, we will discuss more details of the most common metrics.\n",
    "\n",
    "### Mean Absolute Error (MAE)\n",
    "\n",
    "MAE is a very simple metric which calculates the absolute difference between actual and predicted values. It has the advantage that the MAE you get is in the same unit as the output variable.\n",
    "\n",
    "In contrast, the disadvantage is that the graph of MAE is not differentiable so we have to apply various optimizers like gradient descent which can be differentiable.\n",
    "\n",
    "This metric is described with the following equation:\n",
    "\n",
    "\\begin{equation}\n",
    "MAE = \\frac{1}{n} \\sum_{1=n}^{i=1} \\left|h(x_n) - y_n\\right|\n",
    "\\end{equation}\n",
    "\n",
    "### Mean Squared Error (MSE)\n",
    "\n",
    "It represents the squared distance between actual and predicted values. we perform squared to avoid the cancellation of negative terms and it is the benefit of MSE. MSE is a most used and very simple metric with a little bit of change in mean absolute error. Mean squared error states that finding the squared difference between actual and predicted value.\n",
    "\n",
    "So, above we are finding the absolute difference and here we are finding the squared difference. The graph of MSE is differentiable, so you can easily use it as a loss function.\n",
    "\n",
    "It has the disadvantage that the value you get after calculating MSE is a squared unit of output. For example, the output variable is in meter (m) then after calculating MSE the output we get is in meter squared and if the data has outliers t then it penalizes the outliers most and the calculated MSE is bigger. So, in short, it is not robust to outliers which were an advantage in MAE.\n",
    "\n",
    "This metric is described with the following equation:\n",
    "\n",
    "\\begin{equation}\n",
    "MSE = \\frac{1}{n} \\sum_{1=n}^{i=1} \\left(h(x_n) - y_n\\right)^2\n",
    "\\end{equation}\n",
    "\n",
    "### Root Mean Squared Error (RMSE)\n",
    "\n",
    "As RMSE is clear by the name itself, that it is a simple square root of mean squared error. It has the advantage that the output value you get is in the same unit as the required output variable which makes interpretation of loss easy, but it is not that robust to outliers as compared to MAE.\n",
    "\n",
    "This metric is described with the following equation:\n",
    "\n",
    "\\begin{equation}\n",
    "RMSE = \\sqrt{\\frac{1}{n} \\sum_{1=n}^{i=1} \\left(h(x_n) - y_n\\right)^2}\n",
    "\\end{equation}\n",
    "\n",
    "### Root Mean Squared Logarithmic Error (RMSLE)\n",
    "\n",
    "Taking the $\\log$ of the RMSE metric slows down the scale of error. The metric is very helpful when you are developing a model without calling the inputs. In that case, the output will vary on a large scale. To control this situation of RMSE we take the $\\log$  of calculated RMSE error and resultant we get as RMSLE.\n",
    "\n",
    "This metric is described with the following equation:\n",
    "\n",
    "\\begin{equation}\n",
    "RMSLE = \\sqrt{\\frac{1}{n} \\sum_{1=n}^{i=1} \\left( \\log \\left(h(x_n) + 1\\right) - \\log \\left( y_n + 1 \\right) \\right)^2}\n",
    "\\end{equation}\n",
    "\n",
    "### R Squared ($R^2$)\n",
    "\n",
    "$R^2$ score is a metric that tells the performance of your model, not the loss in an absolute sense that how many wells did your model perform. In contrast, MAE and MSE depend on the context as we have seen whereas the R2 score is independent of context.\n",
    "\n",
    "So, with help of R squared we have a baseline model to compare a model which none of the other metrics provides. The same we have in classification problems which we call a threshold which is fixed at 0.5. So basically R2 squared calculates how must regression line is better than a mean line.\n",
    "\n",
    "This metric is described with the following equation:\n",
    "\n",
    "\\begin{equation}\n",
    "R^2 = 1 - \\frac{RSS}{TSS}\n",
    "\\end{equation}\n",
    "\n",
    "Where RSS is the Residual Sum of Squares and TSS is the Total Sum of Squares. They are described with the following formulas:\n",
    "\n",
    "\\begin{equation}\n",
    "RSS =  \\sum_{1=n}^{i=1} \\left(h(x_n) - y_n\\right)^2\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "TSS =  \\sum_{1=n}^{i=1} \\left(h(x_n) - \\overline{y}\\right)^2\n",
    "\\end{equation}\n",
    "\n",
    "### Adjusted R Squared\n",
    "\n",
    "The disadvantage of the R2 score is while adding new features in data the R2 score starts increasing or remains constant but it never decreases because It assumes that while adding more data variance of data increases.\n",
    "\n",
    "But the problem is when we add an irrelevant feature in the dataset then at that time R2 sometimes starts increasing which is incorrect. Hence, To control this situation adjusted R Squared came into existence.\n",
    "\n",
    "This metric is described with the following equation:\n",
    "\n",
    "\\begin{equation}\n",
    "R^2_a = 1 - \\left| \\left( \\frac{n-1}{n-k-1}\\right) \\times  \\left(1 - R2\\right) \\right|\n",
    "\\end{equation}\n",
    "\n",
    "Where $k$ is the number of independent variables, $n$ is the number of observations, and $R^2$ is the R squared value.\n",
    "\n",
    "**References**: \n",
    "\n",
    "- https://towardsdatascience.com/20-popular-machine-learning-metrics-part-1-classification-regression-evaluation-metrics-1ca3e282a2ce\n",
    "- https://machinelearningmastery.com/regression-metrics-for-machine-learning/\n",
    "- https://www.analyticsvidhya.com/blog/2021/05/know-the-best-evaluation-metrics-for-your-regression-model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1029,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Example of Regression Metrics\n",
    "# Notebook from: https://app.neptune.ai/theaayushbajaj/sandbox/n/f884bbea-5263-4aeb-aa35-18d74b2835b9/41813125-2b9d-4332-b73f-f07c3b977372\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(x, y)\n",
    "y_hat = regressor.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1030,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 3.27 (+/- 3.35)\n"
     ]
    }
   ],
   "source": [
    "mae = np.abs(y-y_hat)\n",
    "\n",
    "print(f\"MAE: {mae.mean():0.2f} (+/- {mae.std():0.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1031,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 21.89 (+/- 59.14)\n"
     ]
    }
   ],
   "source": [
    "mse = (y-y_hat)**2\n",
    "\n",
    "print(f\"MSE: {mse.mean():0.2f} (+/- {mse.std():0.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1032,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 4.68\n"
     ]
    }
   ],
   "source": [
    "mse = (y-y_hat)**2\n",
    "\n",
    "rmse = np.sqrt(mse.mean())\n",
    "\n",
    "print(f\"RMSE: {rmse:0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1033,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 coefficient of determination: 74.06%\n"
     ]
    }
   ],
   "source": [
    "# R^2 coefficient of determination\n",
    "SE_line = sum((y-y_hat)**2)\n",
    "SE_mean = sum((y-y.mean())**2)\n",
    "\n",
    "r2 = 1-(SE_line/SE_mean)\n",
    "\n",
    "print(f\"R^2 coefficient of determination: {r2*100:0.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "762531c616344ec89a3ae3efe707c5228b11104740e698ce334efdf481e7418a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

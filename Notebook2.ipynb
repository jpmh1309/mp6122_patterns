{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkQCUpwtdq7L"
      },
      "source": [
        "# Costa Rica Institute of Technology\n",
        "* Course: MP-6122 Pattern Recognition\n",
        "* Student: Jose Martinez Hdez\n",
        "* Year: 2022\n",
        "* Notebook 2: Supervised Learning Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dI6gAS28dq7S"
      },
      "source": [
        "# Importing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7qWiAlVKdq7T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f49b4a3a-1a18-452d-8e2b-4d61bc2b2ea7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Importing libaries \n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import drive, data_table\n",
        "\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.warn(\"this will not show\")\n",
        "\n",
        "data_table.enable_dataframe_formatter()\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-oWCAxDdq7U"
      },
      "source": [
        "## Classification Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qpWZg67Gdq7V",
        "outputId": "d68b79c5-a0d9-4194-df38-aab0f4ea47c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   code_number  Clump_Thickness  Uniformity_of_Cell_Size  \\\n",
              "0      1002945                5                        4   \n",
              "1      1015425                3                        1   \n",
              "2      1016277                6                        8   \n",
              "3      1017023                4                        1   \n",
              "4      1017122                8                       10   \n",
              "\n",
              "   Uniformity_of_Cell_Shape  Marginal_Adhesion  Single_Epithelial_Cell_Size  \\\n",
              "0                         4                  5                            7   \n",
              "1                         1                  1                            2   \n",
              "2                         8                  1                            3   \n",
              "3                         1                  3                            2   \n",
              "4                        10                  8                            7   \n",
              "\n",
              "  Bare_Nuclei  Bland_Chromatin  Normal_Nucleoli  Mitoses  Class  \n",
              "0          10                3                2        1      2  \n",
              "1           2                3                1        1      2  \n",
              "2           4                3                7        1      2  \n",
              "3           1                3                1        1      2  \n",
              "4          10                9                7        1      4  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6678ce8c-3c33-498e-862e-fc2a42305117\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>code_number</th>\n",
              "      <th>Clump_Thickness</th>\n",
              "      <th>Uniformity_of_Cell_Size</th>\n",
              "      <th>Uniformity_of_Cell_Shape</th>\n",
              "      <th>Marginal_Adhesion</th>\n",
              "      <th>Single_Epithelial_Cell_Size</th>\n",
              "      <th>Bare_Nuclei</th>\n",
              "      <th>Bland_Chromatin</th>\n",
              "      <th>Normal_Nucleoli</th>\n",
              "      <th>Mitoses</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1002945</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1015425</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1016277</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1017023</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1017122</td>\n",
              "      <td>8</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>10</td>\n",
              "      <td>9</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6678ce8c-3c33-498e-862e-fc2a42305117')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6678ce8c-3c33-498e-862e-fc2a42305117 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6678ce8c-3c33-498e-862e-fc2a42305117');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/81f25a011006ed3d/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 1002945,\n            'f': \"1002945\",\n        },\n{\n            'v': 5,\n            'f': \"5\",\n        },\n{\n            'v': 4,\n            'f': \"4\",\n        },\n{\n            'v': 4,\n            'f': \"4\",\n        },\n{\n            'v': 5,\n            'f': \"5\",\n        },\n{\n            'v': 7,\n            'f': \"7\",\n        },\n\"10\",\n{\n            'v': 3,\n            'f': \"3\",\n        },\n{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 2,\n            'f': \"2\",\n        }],\n [{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 1015425,\n            'f': \"1015425\",\n        },\n{\n            'v': 3,\n            'f': \"3\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 2,\n            'f': \"2\",\n        },\n\"2\",\n{\n            'v': 3,\n            'f': \"3\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 2,\n            'f': \"2\",\n        }],\n [{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': 1016277,\n            'f': \"1016277\",\n        },\n{\n            'v': 6,\n            'f': \"6\",\n        },\n{\n            'v': 8,\n            'f': \"8\",\n        },\n{\n            'v': 8,\n            'f': \"8\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 3,\n            'f': \"3\",\n        },\n\"4\",\n{\n            'v': 3,\n            'f': \"3\",\n        },\n{\n            'v': 7,\n            'f': \"7\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 2,\n            'f': \"2\",\n        }],\n [{\n            'v': 3,\n            'f': \"3\",\n        },\n{\n            'v': 1017023,\n            'f': \"1017023\",\n        },\n{\n            'v': 4,\n            'f': \"4\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 3,\n            'f': \"3\",\n        },\n{\n            'v': 2,\n            'f': \"2\",\n        },\n\"1\",\n{\n            'v': 3,\n            'f': \"3\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 2,\n            'f': \"2\",\n        }],\n [{\n            'v': 4,\n            'f': \"4\",\n        },\n{\n            'v': 1017122,\n            'f': \"1017122\",\n        },\n{\n            'v': 8,\n            'f': \"8\",\n        },\n{\n            'v': 10,\n            'f': \"10\",\n        },\n{\n            'v': 10,\n            'f': \"10\",\n        },\n{\n            'v': 8,\n            'f': \"8\",\n        },\n{\n            'v': 7,\n            'f': \"7\",\n        },\n\"10\",\n{\n            'v': 9,\n            'f': \"9\",\n        },\n{\n            'v': 7,\n            'f': \"7\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 4,\n            'f': \"4\",\n        }]],\n        columns: [[\"number\", \"index\"], [\"number\", \"code_number\"], [\"number\", \"Clump_Thickness\"], [\"number\", \"Uniformity_of_Cell_Size\"], [\"number\", \"Uniformity_of_Cell_Shape\"], [\"number\", \"Marginal_Adhesion\"], [\"number\", \"Single_Epithelial_Cell_Size\"], [\"string\", \"Bare_Nuclei\"], [\"number\", \"Bland_Chromatin\"], [\"number\", \"Normal_Nucleoli\"], [\"number\", \"Mitoses\"], [\"number\", \"Class\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n    "
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/MP6122/dataset/breast-cancer-wisconsin.data')\n",
        "df.columns = ['code_number', 'Clump_Thickness','Uniformity_of_Cell_Size' ,'Uniformity_of_Cell_Shape', 'Marginal_Adhesion',\n",
        "              'Single_Epithelial_Cell_Size',  'Bare_Nuclei', 'Bland_Chromatin', 'Normal_Nucleoli' ,'Mitoses','Class']\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kK0-o4B0dq7Y"
      },
      "outputs": [],
      "source": [
        "df = df.replace('?', np.nan)\n",
        "df.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "t71CXwcfdq7Y"
      },
      "outputs": [],
      "source": [
        "df.Class = df.Class.map({2: 0, 4:1})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GqkJdZGhdq7Z",
        "outputId": "bfd94f1a-9de8-4406-84fb-d413d1744f64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f1e391709d0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "df.Class.value_counts().plot.bar()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TbQTEQsAdq7a"
      },
      "outputs": [],
      "source": [
        "X = df.drop(['code_number','Class'],axis=1).values\n",
        "Y = df.Class.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "S9meq1l2dq7b",
        "outputId": "872fe580-6b5e-4dcf-ee0f-d960a405d126",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, random_state=42)\n",
        "df.Class.unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1PIwlk6dq7b"
      },
      "source": [
        "## Regression Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zaMrdi90dq7c",
        "outputId": "d8e0ae85-0ec7-4e9a-ae6a-ff4372852626",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   code_number  Clump_Thickness  Uniformity_of_Cell_Size  \\\n",
              "0      1002945                5                        4   \n",
              "1      1015425                3                        1   \n",
              "2      1016277                6                        8   \n",
              "3      1017023                4                        1   \n",
              "4      1017122                8                       10   \n",
              "\n",
              "   Uniformity_of_Cell_Shape  Marginal_Adhesion  Single_Epithelial_Cell_Size  \\\n",
              "0                         4                  5                            7   \n",
              "1                         1                  1                            2   \n",
              "2                         8                  1                            3   \n",
              "3                         1                  3                            2   \n",
              "4                        10                  8                            7   \n",
              "\n",
              "  Bare_Nuclei  Bland_Chromatin  Normal_Nucleoli  Mitoses  Class  \n",
              "0          10                3                2        1      0  \n",
              "1           2                3                1        1      0  \n",
              "2           4                3                7        1      0  \n",
              "3           1                3                1        1      0  \n",
              "4          10                9                7        1      1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-46064e12-3ee5-43c0-8b11-8e46c896d1e8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>code_number</th>\n",
              "      <th>Clump_Thickness</th>\n",
              "      <th>Uniformity_of_Cell_Size</th>\n",
              "      <th>Uniformity_of_Cell_Shape</th>\n",
              "      <th>Marginal_Adhesion</th>\n",
              "      <th>Single_Epithelial_Cell_Size</th>\n",
              "      <th>Bare_Nuclei</th>\n",
              "      <th>Bland_Chromatin</th>\n",
              "      <th>Normal_Nucleoli</th>\n",
              "      <th>Mitoses</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1002945</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1015425</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1016277</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1017023</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1017122</td>\n",
              "      <td>8</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>10</td>\n",
              "      <td>9</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-46064e12-3ee5-43c0-8b11-8e46c896d1e8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-46064e12-3ee5-43c0-8b11-8e46c896d1e8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-46064e12-3ee5-43c0-8b11-8e46c896d1e8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/81f25a011006ed3d/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 1002945,\n            'f': \"1002945\",\n        },\n{\n            'v': 5,\n            'f': \"5\",\n        },\n{\n            'v': 4,\n            'f': \"4\",\n        },\n{\n            'v': 4,\n            'f': \"4\",\n        },\n{\n            'v': 5,\n            'f': \"5\",\n        },\n{\n            'v': 7,\n            'f': \"7\",\n        },\n\"10\",\n{\n            'v': 3,\n            'f': \"3\",\n        },\n{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 1015425,\n            'f': \"1015425\",\n        },\n{\n            'v': 3,\n            'f': \"3\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 2,\n            'f': \"2\",\n        },\n\"2\",\n{\n            'v': 3,\n            'f': \"3\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': 1016277,\n            'f': \"1016277\",\n        },\n{\n            'v': 6,\n            'f': \"6\",\n        },\n{\n            'v': 8,\n            'f': \"8\",\n        },\n{\n            'v': 8,\n            'f': \"8\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 3,\n            'f': \"3\",\n        },\n\"4\",\n{\n            'v': 3,\n            'f': \"3\",\n        },\n{\n            'v': 7,\n            'f': \"7\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 3,\n            'f': \"3\",\n        },\n{\n            'v': 1017023,\n            'f': \"1017023\",\n        },\n{\n            'v': 4,\n            'f': \"4\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 3,\n            'f': \"3\",\n        },\n{\n            'v': 2,\n            'f': \"2\",\n        },\n\"1\",\n{\n            'v': 3,\n            'f': \"3\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 4,\n            'f': \"4\",\n        },\n{\n            'v': 1017122,\n            'f': \"1017122\",\n        },\n{\n            'v': 8,\n            'f': \"8\",\n        },\n{\n            'v': 10,\n            'f': \"10\",\n        },\n{\n            'v': 10,\n            'f': \"10\",\n        },\n{\n            'v': 8,\n            'f': \"8\",\n        },\n{\n            'v': 7,\n            'f': \"7\",\n        },\n\"10\",\n{\n            'v': 9,\n            'f': \"9\",\n        },\n{\n            'v': 7,\n            'f': \"7\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        }]],\n        columns: [[\"number\", \"index\"], [\"number\", \"code_number\"], [\"number\", \"Clump_Thickness\"], [\"number\", \"Uniformity_of_Cell_Size\"], [\"number\", \"Uniformity_of_Cell_Shape\"], [\"number\", \"Marginal_Adhesion\"], [\"number\", \"Single_Epithelial_Cell_Size\"], [\"string\", \"Bare_Nuclei\"], [\"number\", \"Bland_Chromatin\"], [\"number\", \"Normal_Nucleoli\"], [\"number\", \"Mitoses\"], [\"number\", \"Class\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n    "
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
        "data = pd.read_csv('/content/drive/MyDrive/MP6122/dataset/housing.csv', header=None, delimiter=r\"\\s+\", names=column_names)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "PlM2kqrDdq7c"
      },
      "outputs": [],
      "source": [
        "column_sels = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
        "x = data.loc[:,column_sels]\n",
        "y = data['MEDV']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "57UDcTQudq7d"
      },
      "outputs": [],
      "source": [
        "min_max_scaler = MinMaxScaler()\n",
        "x = pd.DataFrame(data=min_max_scaler.fit_transform(x), columns=column_sels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdJP1066dq7d"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQHCi4asdq7d"
      },
      "source": [
        "## Linear Regression (Regression)\n",
        "\n",
        "### Conceptual Explanation \n",
        "\n",
        "Linear Regression is the supervised machine learning model in which the model finds the best fit linear line between the independent and dependent variable i.e it finds the linear relationship between the dependent and independent variable.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=11R83vkY9NU7-fVvjg8DbkM14dzNhWS1H\" width=400 height=400 />\n",
        "\n",
        "### Mathematical \n",
        "\n",
        "From basic linear algebra, we should remember that the linear regression model is a linear combination of the independent variables, that is described by the following equation: \n",
        "\n",
        "\\begin{equation}\n",
        "y = b_0 + b_1 \\cdot x\n",
        "\\end{equation}\n",
        "\n",
        "Where $b_0$ is the intercept and $b_1$ is the slope.\n",
        "\n",
        "So, the main idea of linear regression is to find the best fit line between the independent and dependent variables. This can be done by finding the value of the slope ($b_1$) and the intercept ($b_0$) in the linear equation that reduces the error between a predicted value from the model and the actual value from the data. \n",
        "\n",
        "We create a hipotetic model by using the following equation:\n",
        "\n",
        "\\begin{equation}\n",
        "h(x) = b_0 + b_1 \\cdot x\n",
        "\\end{equation}\n",
        "\n",
        "And then we calculate the error between the actual value and the predicted value by using the following cost equation of the squared error:\n",
        "\n",
        "\\begin{equation}\n",
        "J(b_0, b_1) = \\frac{1}{2m} \\sum_{1=n}^{i=1} (h(x_n) - y_n)^2\n",
        "\\end{equation}\n",
        "\n",
        "### Optimal Applications \n",
        "\n",
        "This model has the following assumptions and limitations: \n",
        "\n",
        "1. *Linearity*: The relationship between the dependent and independent variables is linear.\n",
        "2. *Homoscedasticity*: The error is homoscedastic. The variance of the error terms should be constant i.e the spread of residuals should be constant for all values of X. This assumption can be checked by plotting a residual plot. If the assumption is violated then the points will form a funnel shape otherwise they will be constant.\n",
        "3. *Independence*:  The variables should be independent of each other i.e no correlation should be there between the independent variables.\n",
        "4. *Normality*: The $x$ and $y$ variables should be normally distributed.\n",
        "\n",
        "Basically, this model will work well when the assumptions are satisfied. The violation of the assumptions leads to a decrease in the accuracy of the model therefore the predictions are not accurate and error is also high.\n",
        "\n",
        "**References**: \n",
        "\n",
        "- https://towardsdatascience.com/ensemble-models-5a62d4f4cb0chttps://www.analyticsvidhya.com/blog/2021/05/all-you-need-to-know-about-your-first-machine-learning-model-linear-regression/#:~:text=In%20the%20most%20simple%20words,the%20dependent%20and%20independent%20variable.\n",
        "- https://www.youtube.com/watch?v=1-OGRohmH2s\n",
        "- https://towardsdatascience.com/mathematics-for-machine-learning-linear-regression-least-square-regression-de09cf53757c\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
        "- https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0mcPGRDDdq7e",
        "outputId": "1ded2ad8-c9b3-4315-8b96-060aa7ebc849",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7406426641094095\n",
            "[ -9.60975755   4.64204584   0.56083933   2.68673382  -8.63457306\n",
            "  19.88368651   0.06721501 -16.22666104   7.03913802  -6.46332721\n",
            "  -8.95582398   3.69282735 -19.01724361]\n",
            "26.620267584687767\n"
          ]
        }
      ],
      "source": [
        "# Example of Linear Regression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "regressor = LinearRegression()\n",
        "regressor.fit(x, y)\n",
        "print(regressor.score(x, y))\n",
        "print(regressor.coef_)\n",
        "print(regressor.intercept_)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGXa44cgdq7f"
      },
      "source": [
        "## Logistic Regression (Classification)\n",
        "\n",
        "### Conceptual Explanation \n",
        "\n",
        "Logistic regression, despite its name, is a classification model rather than regression model. Logistic regression is a simple and more efficient method for binary and linear classification\n",
        "problems. It is a classification model, which is very easy to realize and achieves very good performance with linearly separable classes. It belongs to the group of linear classifiers and is somewhat similar to polynomial and linear regression. Logistic regression is fast and relatively uncomplicated, and it’s convenient for you to interpret the results. Although it’s essentially a method for binary classification, it can also be applied to multiclass problems.\n",
        "\n",
        "![sigmoid](https://drive.google.com/uc?id=11i8CA3Rw1NNFf5HlQ2kfVyJFZ4vBJ_U3)\n",
        "\n",
        "### Mathematical \n",
        "\n",
        "The logistic regression model is a linear combination of the independent variables, that is described by the following equation:\n",
        "\n",
        "\\begin{equation}\n",
        "f(x) = \\frac{1}{1 + e^{-m \\cdot x + b}}\n",
        "\\end{equation}\n",
        "\n",
        "That function is also called the sigmoid function. A sigmoid function is a mathematical function having a characteristic \"S\"-shaped curve or sigmoid curve. The sigmoid function has values very close to either 0 or 1 across most of its domain. This fact makes it suitable for application in classification methods.\n",
        "\n",
        "In this model, we have the linear combination of the independent variables, that is described by the following equation:\n",
        "\n",
        "\\begin{equation}\n",
        "h(x) = b_0 + b_1 \\cdot x_1 + b_2 \\cdot x_2 + ... + b_n \\cdot x_n\n",
        "\\end{equation}\n",
        "\n",
        "That función is called the *logit*. The variables $b_0, b_1, b_2, ..., b_n$ are the coefficients or preticted weights of the linear combination that comes from the $-m \\cdot x + b$ term of sigmoid function. Similar as in linear regression, the idea is to create a cost function that determines the best predicted weights $b_0, b_1, b_2, ..., b_n$  such that the function $f(x)$ is as close as possible to all actual responses $𝑦_i$ values. \n",
        "\n",
        "\\begin{equation}\n",
        "J(b_0, b_1, .., b_n) = \\sum_{1=n}^{i=1} y_n\\log(f(x_n)) + (1-y_n)\\log(1-f(x_n))\n",
        "\\end{equation}\n",
        "\n",
        "The idea of using logarithms is to simplify the cost function. By minimizing the cost function, we can find the best fit line between the independent and dependent variables. The most common way to solve the logistic regression problem is to use the gradient descent algorithm and also with the maximum likelihood estimator (MLE). The main aim of MLE is to find the value of our parameters for which the likelihood function is maximized. The likelihood function is nothing but a joint pdf of our sample observations and joint distribution is the multiplication of the conditional probability for observing each example given the distribution parameters. \n",
        "\n",
        "Gradient descent changes the value of our weights in such a way that it always converges to minimum point or we can also say that, it aims at finding the optimal weights which minimize the loss function of our model. It is an iterative method that finds the minimum of a function by figuring out the slope at a random point and then moving in the opposite direction.\n",
        "\n",
        "### Optimal Applications\n",
        "\n",
        "Logistic regresssion performs well for binary classification problems. It is also a good model for classification problems where the dependent variable is categorical. It has the advantage of being fast and easy to interpret.\n",
        "\n",
        "Lastly, the most significant advantage of logistic regression over other algoritmhs is transparency. Complex models, as neural networks, work as a black box - you never know why it makes one or another decision. There are a lot of highly regulated industries where this approach is not acceptable. Logistic regression, in contrast, may be called the “white box”. You always know why you rejected a loan application or why your patient’s diagnosis looks good or bad. That is what we’ll talk about in detail.\n",
        "\n",
        "**References**: \n",
        "\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
        "- Practical Machine Learning for Data Analysis Using Python\n",
        "- Machine Learning Guide for Oil and Gas Using Python\n",
        "- https://realpython.com/logistic-regression-python/#logistic-regression-overview\n",
        "- https://en.wikipedia.org/wiki/Sigmoid_function\n",
        "- https://www.analyticsvidhya.com/blog/2021/08/conceptual-understanding-of-logistic-regression-for-data-science-beginners/#:~:text=Logistic%20Regression%20is%20another%20statistical,pass%20this%20exam%20or%20not\n",
        "- https://medium.com/analytics-vidhya/logistic-regression-b35d2801a29c\n",
        "- https://activewizards.com/blog/5-real-world-examples-of-logistic-regression-application"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "7zSOYvhedq7g",
        "outputId": "fbd7c622-85a7-4633-e232-16e29a2e2a3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score is 0.9562043795620438\n",
            "[0 1]\n",
            "[[0.37455966 0.21822076 0.50280676 0.2547255  0.02534985 0.34896106\n",
            "  0.29500212 0.23440608 0.46061253]]\n",
            "[-9.5864799]\n"
          ]
        }
      ],
      "source": [
        "# Example of Logistic Regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf = LogisticRegression(random_state=0)\n",
        "clf.fit(X_train, y_train)\n",
        "y_hat = clf.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "print(f'Accuracy Score is {accuracy_score(y_test, y_hat)}')\n",
        "\n",
        "print(clf.classes_)\n",
        "print(clf.coef_)\n",
        "print(clf.intercept_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ia33g54dq7h"
      },
      "source": [
        "## Support Vector Machines (Classification)\n",
        "\n",
        "### Conceptual Explanation \n",
        "\n",
        "Support Vector Machine (SVM) is a supervised machine learning algorithm that can be used for both classification or regression challenges. However,  it is mostly used in classification problems. In the SVM algorithm, we plot each data item as a point in n-dimensional space (where n is a number of features you have) with the value of each feature being the value of a particular coordinate. Then, we perform classification by finding the hyper-plane that differentiates the two classes very well, as it could be seen in the following figure.\n",
        "\n",
        "![svm](https://drive.google.com/uc?id=11U2Z1d4WNfR7gr8p6JMevcyJ1BNyPOBD)\n",
        "\n",
        "### Mathematical \n",
        "\n",
        "The main objective of SVM is to find the optimal hyperplane which linearly separates the data points in two component by maximizing the margin.\n",
        "\n",
        "A hyper-plane means plane that linearly divide the n-dimensional data points in two component. In case of 2D, hyperplane is line, in case of 3D it is plane.It is also called as n-dimensional line. That hyper-plane is defined by the following equation, for the case of 2D:\n",
        "\n",
        "\\begin{equation}\n",
        "H(x): w\\cdot x + b=0\n",
        "\\end{equation}\n",
        "\n",
        "SVM problem can be formulated as:\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{matrix}\n",
        "w \\cdot x_i + b \\geq  1 & y_i = +1 \\\\\n",
        "w \\cdot x_i + b \\leq  -1 & y_i = -1 \\\\\n",
        "\\end{matrix}\n",
        "\\end{equation}\n",
        "\n",
        "Combined with the above equation, we can rewrite the equation as:\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{matrix}\n",
        "y_i (w \\cdot x_i + b) - 1 \\geq 0 & y_i = +1, -1 \\\\\n",
        "\\end{matrix}\n",
        "\\end{equation}\n",
        "\n",
        "The distance between a point and a line can be calculated as:\n",
        "\n",
        "\\begin{equation}\n",
        "d =\\frac{\\left| a \\cdot x_0 + b \\cdot y_0 +c \\right|}{\\sqrt{a^2 + b^2}}\n",
        "\\end{equation}\n",
        "\n",
        "For a hyper-plane, the distance between a point and a line can be calculated as:\n",
        "\n",
        "\\begin{equation}\n",
        "d_H =\\frac{\\left| w(x_0) + b \\right|}{\\left|\\left| w \\right| \\right|}\n",
        "\\end{equation}\n",
        "\n",
        "Since the idea is to maximize the distance, we should minimize the euclidean distance. So, we can rewrite the equation as:\n",
        "\n",
        "\\begin{equation}\n",
        "\\left|\\left| w \\right| \\right| = \\sqrt{\\sum_{j=1}^{D} w_j^2}\n",
        "\\end{equation}\n",
        "\n",
        "Finally, the optimization criterion is:\n",
        "\n",
        "\\begin{equation}\n",
        "min \\left|\\left| w \\right| \\right| = min \\frac{1}{2} \\left|\\left| w \\right| \\right|^2 \n",
        "\\end{equation}\n",
        "\n",
        "What if data points is not linearly separable?\n",
        "\n",
        "In such cases, we do not see a straight line frontier directly in current plane which can serve as the SVM. In such cases, we need to map these vector to a higher dimension plane so that they get segregated from each other, that algorithm is called kernel SVM, which well be covered in the next sections.\n",
        "\n",
        "### Optimal Applications\n",
        "\n",
        "Some of the pros of SVM are:\n",
        "\n",
        "1. It works really well with a clear margin of separation.\n",
        "2. It is effective in high dimensional spaces.\n",
        "3. It is effective in cases where the number of dimensions is greater than the number of samples.\n",
        "4. It uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
        "\n",
        "In the other hand, some of the cons of SVM are:\n",
        "\n",
        "1. It doesn’t perform well when we have large data set because the required training time is higher.\n",
        "2. It also doesn’t perform very well, when the data set has more noise i.e. target classes are overlapping.\n",
        "3. SVM doesn’t directly provide probability estimates, these are calculated using an expensive five-fold cross-validation. \n",
        "\n",
        "\n",
        "Some aplications of SVM are:\n",
        "\n",
        "1. Handwriting recognition\n",
        "2. Intrusion detection \n",
        "3. Face detection\n",
        "4. Email classification \n",
        "5. Gene classification\n",
        "\n",
        "**References**: \n",
        "\n",
        "- https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47\n",
        "- https://scikit-learn.org/stable/modules/classes.html#module-sklearn.svm\n",
        "- https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/\n",
        "- https://www.analyticsvidhya.com/blog/2014/10/support-vector-machine-simplified/?utm_source=blog&utm_medium=understandingsupportvectormachinearticle\n",
        "- https://www.analyticsvidhya.com/blog/2020/10/the-mathematics-behind-svm/#:~:text=A%20Support%20Vector%20Machine%20or,to%20as%20Support%20Vector%20Classification.\n",
        "- https://ankitnitjsr13.medium.com/math-behind-support-vector-machine-svm-5e7376d0ee4d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "H71FmqV3dq7j",
        "outputId": "4834823d-643c-4091-adea-b2a99863045a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score is 0.9635036496350365\n"
          ]
        }
      ],
      "source": [
        "# Example of SVM\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "clf = SVC(kernel='linear')\n",
        "clf.fit(X_train, y_train)\n",
        "y_hat = clf.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "print(f'Accuracy Score is {accuracy_score(y_test, y_hat)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vlx0AFCdq7l"
      },
      "source": [
        "## Linear Models for multi-class classification (Classification)\n",
        "\n",
        "### Conceptual Explanation \n",
        "\n",
        "The idea in this algoritmh is to reuse the 2-class formulation for k-classes.\n",
        "\n",
        "![multi-class](https://drive.google.com/uc?id=11o2jjqqHtnhqVCHtg7ypKatkfeFJowDO)\n",
        "\n",
        "One-vs-Rest (OvR): for each class k train a linear model (SVM or Logistic Regression) that is expert in classifying k (one) versus non-k (the rest) meaning we have to create a binary classifier for each class k, which is an expert in separating that class(k) from all the other classes.\n",
        "\n",
        "One-vs-One (OvO): : for each pair of classes $k$, $k'$), train an SVM that is expert in classifying k versus $k'$.\n",
        "\n",
        "In either case, make multiclass predictions by combining individual binary predictions.\n",
        "\n",
        "There are many more multi-class classifiers available like k-NN, naive bayes, decision trees, etc that will be discussed in the next section.\n",
        "\n",
        "### Mathematical \n",
        "\n",
        "This algoritm uses any other binary linear models (SVM or Logistic Regression) to make predictions for each class and then result is combined to make a final model.\n",
        "\n",
        "### Optimal Applications\n",
        "\n",
        "Some of the advantages of the multi-class classification are:\n",
        "\n",
        "1. Linear models are fast to train and make predictions.\n",
        "2. They scale very well for large or sparse data sets.\n",
        "3. They are easy to understand.\n",
        "\n",
        "**References**: \n",
        "\n",
        "- https://scikit-learn.org/stable/modules/classes.html#module-sklearn.multiclass\n",
        "- https://medium.com/analytics-vidhya/ml06-intro-to-multi-class-classification-e61eb7492ffd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "sb5UuKJndq7l",
        "outputId": "4db10751-6fa9-4d3a-bd55-0f9da9d2a3cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score is 0.9635036496350365\n"
          ]
        }
      ],
      "source": [
        "# Example of Linear Models for multi-class classification\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "clf = OneVsRestClassifier(LinearSVC(random_state=0))\n",
        "clf.fit(X_train, y_train)\n",
        "y_hat = clf.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "print(f'Accuracy Score is {accuracy_score(y_test, y_hat)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ndVrgh1dq7m"
      },
      "source": [
        "## kNN (Classification or Regression)) \n",
        "\n",
        "### Conceptual Explanation \n",
        "\n",
        "The k-means clustering algorithm is one of the most widely used, effective, and best understood clustering methods.\n",
        "\n",
        "![knn](https://drive.google.com/uc?id=11mXbIy1CxEaD1y7tcV0UFEarJmkXJeST)\n",
        "\n",
        "### Mathematical \n",
        "\n",
        "In this case, the key is to find the k nearest neighbors of a given point. For that, we need to calculate the distance between the point and all the other points. The most common distance metrics are:\n",
        "\n",
        "1. Euclidean distance:\n",
        "\n",
        "\\begin{equation}\n",
        "d =\\sqrt{\\sum_{j=1}^{D} (x_j - y_j)^2}\n",
        "\\end{equation}\n",
        "\n",
        "2. Manhattan distance:\n",
        "\n",
        "\\begin{equation}\n",
        "d =\\sum_{j=1}^{D} |x_j - y_j|\n",
        "\\end{equation}\n",
        "\n",
        "3. Cosine distance:\n",
        "\\begin{equation}\n",
        "d =\\frac{1}{\\sqrt{\\sum_{j=1}^{D} x_j^2 \\cdot y_j^2}} \\cdot \\left( \\sum_{j=1}^{D} x_j y_j \\right)\n",
        "\\end{equation}\n",
        "\n",
        "4. Squared Euclidean distance: \n",
        "\n",
        "\\begin{equation}\n",
        "d =\\sum_{j=1}^{D} (x_j - y_j)^2\n",
        "\\end{equation}\n",
        "\n",
        "5. Chebyshev distance:\n",
        "\n",
        "\\begin{equation}\n",
        "d =\\max_{j=1}^{D} |x_j - y_j|\n",
        "\\end{equation}\n",
        "\n",
        "6. Mahalanobis distance:\n",
        "\n",
        "\\begin{equation}\n",
        "d =\\sqrt{\\sum_{j=1}^{D} (x_j - y_j)^2 \\cdot \\left( \\sum_{j=1}^{D} x_j y_j \\right)}\n",
        "\\end{equation}\n",
        "\n",
        "So basically, we create clusters of points and then find the k nearest neighbors of a given point by using any of the distances described above, and in that way we can find the class of the point.\n",
        "\n",
        "One of the most challenging tasks in this algorithm is to choose the right values of $k$. What should be the right $k$-value? How to choose the $k$-value? Let us find the answer to these questions. If you are choosing the $k$ values randomly, it might be correct or may be wrong. If you will choose the wrong value then it will directly affect your model performance. So there are two methods by which you can select the right value of $k$: \n",
        "\n",
        "1. The elbow method\n",
        "\n",
        "2. The silhouette method \n",
        "\n",
        "### Optimal Applications\n",
        "\n",
        "The pros of k-NN are:\n",
        "\n",
        "1. It is very simple to implement.\n",
        "2. It is scalable to a huge data set and also faster to large datasets.\n",
        "3. It adapts the new examples very frequently.\n",
        "4. Generalization of clusters for different shapes and sizes.\n",
        "\n",
        "This cons of k-NN are:\n",
        "\n",
        "1. It is sensitive to the outliers.\n",
        "2. Choosing the k values manually is a tough job.\n",
        "3. As the number of dimensions increases its scalability decreases.\n",
        "\n",
        "Some of the applications of k-NN are:\n",
        "\n",
        "1. It is used for classification.\n",
        "2. It is used for regression.\n",
        "3. It is used for clustering.\n",
        "4. It is used for dimensionality reduction.\n",
        "\n",
        "**References**: \n",
        "\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html\n",
        "- https://towardsdatascience.com/understanding-k-means-clustering-in-machine-learning-6a6e67336aa1\n",
        "- https://www.analyticsvidhya.com/blog/2020/10/a-simple-explanation-of-k-means-clustering/\n",
        "- https://www.simplilearn.com/tutorials/machine-learning-tutorial/k-means-clustering-algorithm\n",
        "- https://www.cs.cornell.edu/~tomf/publications/supervised_kmeans-08.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "-iMpWlLkdq7m",
        "outputId": "f6382c51-b9d5-4d98-a138-5ebb174aafdb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score is 0.9562043795620438\n"
          ]
        }
      ],
      "source": [
        "# Example of KNN Classifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "clf = KNeighborsClassifier(n_neighbors=3)\n",
        "clf.fit(X_train, y_train)\n",
        "y_hat = clf.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "print(f'Accuracy Score is {accuracy_score(y_test, y_hat)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Sizci51Sdq7n",
        "outputId": "bcf322fc-ba94-4505-b16e-355f69cf0167",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.850617542134604\n"
          ]
        }
      ],
      "source": [
        "# Example of kNN Regression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "regressor = KNeighborsRegressor()\n",
        "regressor.fit(x, y)\n",
        "print(regressor.score(x, y))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDtAhNAxdq7n"
      },
      "source": [
        "## Naive Bayes (Classification)\n",
        "\n",
        "### Conceptual Explanation \n",
        "\n",
        "It is a classification technique based on Bayes' Theorem with an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature.\n",
        "\n",
        "![naive](https://drive.google.com/uc?id=11xgMrqGRk3NIiIki7Mjj1Dg7nS7E3D05)\n",
        "\n",
        "### Mathematical \n",
        "\n",
        "The Bayes' Theorem states that the probability of an event occurring is a function of the probability of that event and the probability of that event occurring given the evidence that led to its occurrence, that is discribed by the following formula:\n",
        "\n",
        "\\begin{equation}\n",
        "P(C|x) = \\frac{P(x|C)P(C)}{P(x)}\n",
        "\\end{equation}\n",
        "\n",
        "Where:\n",
        "\n",
        "- $P(C|x)$ is the probability of the class $C$ given the evidence $x$.\n",
        "- $P(x|C)$ is the probability of the evidence $x$ given the class $C$.\n",
        "- $P(C)$ is the probability of the class $C$.\n",
        "- $P(x)$ is the probability of the evidence $x$.\n",
        "\n",
        "Using the chain rule, the likelihood P(X∣Ck) can be decomposed as:\n",
        "\n",
        "\\begin{equation}\n",
        "P(x|Ck) = P(x_1, x_2, ..., x_n|Ck)P(Ck) = P(x_1|x_2, ..., x_n, C_k)P(x_2|x_1, ..., x_n, C_k)...P(x_n|x_1, ..., x_n-1, C_k)P(Ck)\n",
        "\\end{equation}\n",
        "\n",
        "The above sets of probabilities can be hard and expensive to calculate. Fortunately, with the naive conditional independence assumption, this probability can be also expressed as:\n",
        "\n",
        "\\begin{equation}\n",
        "P(C|x) = \\frac{P(C_k) \\prod_{i=1}^{n}P(x_i|C_i)}{P(x)}\n",
        "\\end{equation}\n",
        "\n",
        "So the problem of Naive Bayes is for different class values of $C_k$, find the maximum of $P(C_k) \\prod_{i=1}^{n}P(x_i|C_i)$.\n",
        "\n",
        "There are three types of Naive Bayes classifiers:\n",
        "\n",
        "1. Gaussian Naive Bayes\n",
        "2. Bernoulli Naive Bayes\n",
        "3. Multinomial Naive Bayes\n",
        "\n",
        "The different Naive Bayes classifiers differ mainly by the assumptions they make regarding the distribution of $P(x_i|C_i)$.\n",
        "\n",
        "### Optimal Applications\n",
        "\n",
        "The main advantages of Naive Bayes are: \n",
        "\n",
        "1. It is easy and fast to predict class of test data set. It also perform well in multi class prediction.\n",
        "2. When assumption of independence holds, a Naive Bayes classifier performs better compare to other models like logistic regression and you need less training data.\n",
        "3. It perform well in case of categorical input variables compared to numerical variable(s). For numerical variable, normal distribution is assumed (bell curve, which is a strong assumption).\n",
        "\n",
        "And the main cons of Naive Bayes are:\n",
        "\n",
        "1. It perform well in case of categorical input variables compared to numerical variable(s). For numerical variable, normal distribution is assumed (bell curve, which is a strong assumption).\n",
        "2. On the other side naive Bayes is also known as a bad estimator.\n",
        "3. Another limitation of Naive Bayes is the assumption of independent predictors. In real life, it is almost impossible that we get a set of predictors which are completely independent.\n",
        "\n",
        "Some of the applications of Naive Bayes are:\n",
        "\n",
        "1. Real time Prediction\n",
        "2. Recommendation System\n",
        "3. Sentiment Analysis\n",
        "4. Text Classification\n",
        "5. Image Classification\n",
        "6. Fraud Detection\n",
        "\n",
        "**References**: \n",
        "\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html \n",
        "- https://medium.com/analytics-vidhya/na%C3%AFve-bayes-algorithm-5bf31e9032a2#:~:text=Naive%20Bayes%20is%20a%20classification,one%20with%20the%20highest%20probability.\n",
        "- https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/\n",
        "- https://shuzhanfan.github.io/2018/06/understanding-mathematics-behind-naive-bayes/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47dfFH7udq7n",
        "outputId": "1f49349c-83e4-4e9c-bd50-491311466536"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score is 0.8978102189781022\n"
          ]
        }
      ],
      "source": [
        "# Example of Naive Bayes\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "clf = MultinomialNB(alpha=1.0)\n",
        "clf.fit(X_train, y_train)\n",
        "y_hat = clf.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "print(f'Accuracy Score is {accuracy_score(y_test, y_hat)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbCTMfYWdq7o"
      },
      "source": [
        "## Decision Trees (Classification or Regression)) \n",
        "\n",
        "### Conceptual Explanation \n",
        "\n",
        "A decision tree is a tree-based model for classification and regression. It is a supervised learning algorithm that is used to make predictions by recursively splitting data instances into smaller and smaller groups, each of which is also a decision tree itself. Decision trees imitate human thinking, so it’s generally easy for data scientists to understand and interpret the results.\n",
        "\n",
        "![decision-tree](https://drive.google.com/uc?id=11lITuCVXSwR5TsSU1UJk1hwuk5L7NW2L)\n",
        "\n",
        "Let’s define some key terms of a decision tree.\n",
        "\n",
        "- Root node: The base of the decision tree.\n",
        "- Splitting: The process of dividing a node into multiple sub-nodes.\n",
        "- Decision node: When a sub-node is further split into additional sub-nodes.\n",
        "- Leaf node: When a sub-node does not further split into additional sub-nodes; represents possible outcomes.\n",
        "- Pruning: The process of removing sub-nodes of a decision tree.\n",
        "- Branch: A subsection of the decision tree consisting of multiple nodes.\n",
        "\n",
        "### Mathematical \n",
        "\n",
        "A decision tree resembles, well, a tree. The base of the tree is the root node. From the root node flows a series of decision nodes that depict decisions to be made. From the decision nodes are leaf nodes that represent the consequences of those decisions. Each decision node represents a question or split point, and the leaf nodes that stem from a decision node represent the possible answers. Leaf nodes sprout from decision nodes similar to how a leaf sprouts on a tree branch. This is why we call each subsection of a decision tree a \"branch\".\n",
        "\n",
        "How does a tree based algorithms decide where to split?\n",
        "\n",
        "The decision of making strategic splits heavily affects a tree’s accuracy. The decision criteria is different for classification and regression trees. If the dataset consists of N attributes then deciding which attribute to place at the root or at different levels of the tree as internal nodes is a complicated step. By just randomly selecting any node to be the root can’t solve the issue. If we follow a random approach, it may give us bad results with low accuracy.\n",
        "\n",
        "For solving this attribute selection problem, researchers worked and devised some solutions. They suggested using some criteria like:\n",
        "\n",
        "1. Information Gain\n",
        "\n",
        "\\begin{equation}\n",
        "\\text{Information Gain} = \\text{Entropy}(D) - \\sum_{i=1}^{N}P(i) \\times \\text{Entropy}(D|i)\n",
        "\\end{equation}\n",
        "\n",
        "2. Gini Index:\n",
        "\n",
        "\\begin{equation}\n",
        "Gini(x) = 1 - \\sum_{i=1}^{N}P(x_i)^2\n",
        "\\end{equation}\n",
        "\n",
        "3. Variance Impurity\n",
        "\n",
        "\\begin{equation}\n",
        "\\text{Variance Impurity} = \\sum_{i=1}^{N}P(x_i) \\times \\sum_{i=1}^{N}P(x_i)\n",
        "\\end{equation}\n",
        "\n",
        "4. Entropy\n",
        "\n",
        "\\begin{equation}\n",
        "\\text{Entropy} = - \\sum_{i=1}^{N}P(x_i) \\times \\log(P(x_i))\n",
        "\\end{equation}\n",
        "\n",
        "5. Chi-Square\n",
        "\n",
        "\\begin{equation}\n",
        "\\text{Chi-Square} = \\sum_{i=1}^{N}P(x_i) \\times \\sum_{i=1}^{N}P(x_i)\n",
        "\\end{equation}\n",
        "\n",
        "7. Gain Ratio\n",
        "\n",
        "\\begin{equation}\n",
        "\\text{Gain Ratio} = \\text{Information Gain} / \\text{Variance Impurity}\n",
        "\\end{equation}\n",
        "\n",
        "We should also consider that decision trees are more likely to overfit the data if the number of attributes is large. Overfitting is one of the key challenges faced while using tree based algorithms. If there is no limit set of a decision tree, it will give you 100% accuracy on training set because in the worse case it will end up making 1 leaf for each observation. Thus, preventing overfitting is pivotal while modeling a decision tree and it can be done in 2 ways:\n",
        "\n",
        "1. Pruning\n",
        "2. Setting a maximum depth of the tree\n",
        "\n",
        "### Optimal Applications\n",
        "\n",
        "The pros of decision trees are:\n",
        "\n",
        "1. Works for numerical or categorical data and variables.\n",
        "2. Models problems with multiple outputs.\n",
        "3. Tests the reliability of the tree.\n",
        "4. Requires less data cleaning than other data modeling techniques.\n",
        "5. Easy to explain to those without an analytical background.\n",
        "\n",
        "In the other hand, the cons of decision trees are:\n",
        "\n",
        "1. Affected by noise in the data.\n",
        "2. Not ideal for large datasets.\n",
        "3. Can disproportionately value, or weigh, attributes.\n",
        "4. The decisions at nodes are limited to binary outcomes, reducing the complexity that the tree can handle.\n",
        "5. Trees can become very complex when dealing with uncertainty and numerous linked outcomes.\n",
        "\n",
        "The main applications of decision trees are:\n",
        "\n",
        "1. Classification\n",
        "2. Regression\n",
        "3. Recommendation System\n",
        "4. Sentiment Analysis\n",
        "\n",
        "**References**: \n",
        "\n",
        "- https://www.analyticsvidhya.com/blog/2016/04/tree-based-algorithms-complete-tutorial-scratch-in-python/\n",
        "- https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-understanding-decision-trees-adb2165ccab7\n",
        "- https://www.datacamp.com/community/tutorials/decision-tree-classification-python\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
        "- https://www.mastersindatascience.org/learning/introduction-to-machine-learning-algorithms/decision-tree/#:~:text=A%20decision%20tree%20is%20a,that%20contains%20the%20desired%20categorization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "2LLzunB5dq7o",
        "outputId": "c140c558-82cd-44d9-c0b5-ec420a60eb54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score is 0.9416058394160584\n"
          ]
        }
      ],
      "source": [
        "# Example of Decision Trees Classifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "clf = DecisionTreeClassifier(max_depth=2, random_state=0)\n",
        "clf.fit(X_train, y_train)\n",
        "y_hat = clf.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "print(f'Accuracy Score is {accuracy_score(y_test, y_hat)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "pKiBTsF_dq7p",
        "outputId": "15f895c1-168c-4cd5-be9c-94fa0a0648c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n",
            "[0.03998948 0.00060121 0.00409867 0.00084715 0.05046496 0.57598062\n",
            " 0.0115331  0.07344814 0.00071385 0.01265229 0.02041985 0.00596534\n",
            " 0.20328533]\n"
          ]
        }
      ],
      "source": [
        "# Example of Decision Trees Regression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "regressor = DecisionTreeRegressor(random_state=0)\n",
        "regressor.fit(x, y)\n",
        "print(regressor.score(x, y))\n",
        "\n",
        "print(regressor.feature_importances_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGbGmf1Mdq7p"
      },
      "source": [
        "## Random Forests (Classification or Regression)\n",
        "\n",
        "### Conceptual Explanation \n",
        "\n",
        "A random forest is an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n",
        "\n",
        "![random-forest](https://drive.google.com/uc?id=121o-uPPJl8OXotKBkr_ya75D45C1fSRf)\n",
        "\n",
        "### Mathematical \n",
        "\n",
        "It works in the following manner:\n",
        "\n",
        "1. A random subset of the training data is chosen as the initial set of samples.\n",
        "2. The subset is then split into a number of subsets, each of which is then used to train a decision tree.\n",
        "3. The resulting decision trees are then combined to form a forest. \n",
        "4. The final output is the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n",
        "\n",
        "### Optimal Applications\n",
        "\n",
        "The pros of ramdom forests are:\n",
        "\n",
        "1. This algorithm can solve both type of problems i.e. classification and regression and does a decent estimation at both fronts.\n",
        "2. One of benefits of Random forest which excites me most is, the power of handle large data set with higher dimensionality. It can handle thousands of input variables and identify most significant variables so it is considered as one of the dimensionality reduction methods. Further, the model outputs Importance of variable, which can be a very handy feature (on some random data set).\n",
        "3. It has an effective method for estimating missing data and maintains accuracy when a large proportion of the data are missing.\n",
        "4. It has methods for balancing errors in data sets where classes are imbalanced.\n",
        "5. The capabilities of the above can be extended to unlabeled data, leading to unsupervised clustering, data views and outlier detection.\n",
        "6. Random Forest involves sampling of the input data with replacement called as bootstrap sampling. Here one third of the data is not used for training and can be used to testing. These are called the out of bag samples. Error estimated on these out of bag samples is known as out of bag error. Study of error estimates by Out of bag, gives evidence to show that the out-of-bag estimate is as accurate as using a test set of the same size as the training set. Therefore, using the out-of-bag error estimate removes the need for a set aside test set.\n",
        " \n",
        "In the other hand, the cons of random forest are:\n",
        "\n",
        "1. It surely does a good job at classification but not as good as for regression problem as it does not give precise continuous nature predictions. In case of regression, it doesn’t predict beyond the range in the training data, and that they may over-fit data sets that are particularly noisy.\n",
        "2. Random Forest can feel like a black box approach for statistical modelers – you have very little control on what the model does.\n",
        "\n",
        "**References**: \n",
        "\n",
        "- https://www.analyticsvidhya.com/blog/2016/04/tree-based-algorithms-complete-tutorial-scratch-in-python/\n",
        "- https://www.analyticsvidhya.com/blog/2014/06/introduction-random-forest-simplified/\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "R5PA3aO0dq7p",
        "outputId": "1aaf3111-adf1-4299-82d6-7de91a9362cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score is 0.9562043795620438\n"
          ]
        }
      ],
      "source": [
        "# Example of Random Forest Classifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
        "clf.fit(X_train, y_train)\n",
        "y_hat = clf.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "print(f'Accuracy Score is {accuracy_score(y_test, y_hat)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "T2InVypjdq7q",
        "outputId": "4ecf060f-44d3-4b09-e02e-e1e419507f74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.763042092638991\n"
          ]
        }
      ],
      "source": [
        "# Example of Random Forest Regression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "regressor = RandomForestRegressor(max_depth=2, random_state=0)\n",
        "regressor.fit(x, y)\n",
        "print(regressor.score(x, y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJBrCZ4Sdq7q"
      },
      "source": [
        "## Kernel Support Vector Machines (Classification)\n",
        "\n",
        "### Conceptual Explanation \n",
        "\n",
        "SVM algorithms use a set of mathematical functions that are defined as the kernel. The function of kernel is to take data as input and transform it into the required form. Different SVM algorithms use different types of kernel functions. \n",
        "\n",
        "![kernel](https://drive.google.com/uc?id=11z-qJvrqnyuNqSvin46VaC0H1eZFaud2)\n",
        "\n",
        "Using kernel functions is very useful in classification problems when the data is not linearly separable.\n",
        "\n",
        "### Mathematical \n",
        "\n",
        "Basically, the main idea is first to find the hyperplane that separates the data. Then, the kernel function is used to transform the data into the required form. Some of the kernel functions are:\n",
        "\n",
        "1. Polynomial Kernel: \n",
        "\\begin{equation}\n",
        "K(x,y) = \\alpha \\cdot x^T \\cdot y + \\beta \\cdot x^T \\cdot x + \\gamma \\cdot y^T \\cdot y\n",
        "\\end{equation}\n",
        "\n",
        "2. Gaussian Kernel: \n",
        "\\begin{equation}\n",
        "K(x,y) = \\exp \\left( - \\frac{||x-y||}{\\sigma} \\right)\n",
        "\\end{equation}\n",
        "\n",
        "3. Sigmoid kernel \n",
        "\\begin{equation}\n",
        "K(x,y) = \\frac{1}{1 + e^{-\\gamma \\cdot \\sum_{i=1}^{N}(x_i - y_i)}}\n",
        "\\end{equation}\n",
        "\n",
        "4. Laplace Kernel\n",
        "\n",
        "\\begin{equation}\n",
        "K(x,y) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{1}{2\\sigma^2} \\left(x-y\\right)^2\\right)\n",
        "\\end{equation}\n",
        "\n",
        "5. Bessel Function Kernel\n",
        "\n",
        "\\begin{equation}\n",
        "K(x,y) = \\frac{1}{\\pi\\sigma^2} \\exp\\left(-\\frac{1}{2\\sigma^2} \\left(x-y\\right)^2\\right)\n",
        "\\end{equation}\n",
        "\n",
        "Once the kernel function is found, the SVM algorithm is applied.\n",
        "\n",
        "### Optimal Applications\n",
        "\n",
        "Similar pros and cons of SVM. \n",
        "\n",
        "Some of the pros of SVM are:\n",
        "\n",
        "1. It works really well with a clear margin of separation.\n",
        "2. It is effective in high dimensional spaces.\n",
        "3. It is effective in cases where the number of dimensions is greater than the number of samples.\n",
        "4. It uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
        "\n",
        "In the other hand, some of the cons of SVM are:\n",
        "\n",
        "1. It doesn’t perform well when we have large data set because the required training time is higher.\n",
        "2. It also doesn’t perform very well, when the data set has more noise i.e. target classes are overlapping.\n",
        "3. SVM doesn’t directly provide probability estimates, these are calculated using an expensive five-fold cross-validation. \n",
        "\n",
        "**References**: \n",
        "\n",
        "- https://dataaspirant.com/svm-kernels/\n",
        "- https://data-flair.training/blogs/svm-kernel-functions/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "x6LsJzpMdq7q",
        "outputId": "a38dc690-5978-4033-a773-12bb58d7ebee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score is 0.9635036496350365\n"
          ]
        }
      ],
      "source": [
        "# Example of kernel SVM\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "clf = SVC(kernel='rbf', C=10, gamma=0.1)\n",
        "clf.fit(X_train, y_train)\n",
        "y_hat = clf.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "print(f'Accuracy Score is {accuracy_score(y_test, y_hat)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPTRmSIqdq7r"
      },
      "source": [
        "# Ensemble Methods\n",
        "\n",
        "The goal of ensemble methods is to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability / robustness over a single estimator. \n",
        "\n",
        "![ensemble](https://drive.google.com/uc?id=12-uRKenoVsmQKHFYwRj9Vql61zGws9vk)\n",
        "\n",
        "**References**: \n",
        "\n",
        "- https://towardsdatascience.com/ensemble-models-5a62d4f4cb0c\n",
        "- https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/\n",
        "- https://www.geeksforgeeks.org/ensemble-methods-in-python/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voZVsSvvdq7r"
      },
      "source": [
        "# Simple Ensemble Methods\n",
        "\n",
        "## Voting\n",
        "\n",
        "### Conceptual Explanation \n",
        "\n",
        "It is mainly used for classification problems. The method consists of building multiple models independently and getting their individual output called ‘vote’. The class with maximum votes is returned as output.\n",
        "\n",
        "**References**: \n",
        "\n",
        "- https://www.geeksforgeeks.org/ensemble-methods-in-python/\n",
        "- https://machinelearningmastery.com/ensemble-machine-learning-algorithms-python-scikit-learn/\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingRegressor.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "C_gRGpQ5dq7r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7edac35b-6c23-4dbf-eb67-471be3e7c672"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score is 0.9635036496350365\n",
            "[LogisticRegression(), KNeighborsClassifier(), DecisionTreeClassifier()]\n",
            "{'lr': LogisticRegression(), 'knn': KNeighborsClassifier(), 'dt': DecisionTreeClassifier()}\n",
            "LabelEncoder()\n",
            "hard\n"
          ]
        }
      ],
      "source": [
        "# Example of voting\n",
        "\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Initializing all the model objects with default parameters\n",
        "model_1 = LogisticRegression()\n",
        "model_2 = KNeighborsClassifier()\n",
        "model_3 = DecisionTreeClassifier()\n",
        "\n",
        "# Making the final model using voting classifier\n",
        "final_model = VotingClassifier(estimators=[('lr', model_1), ('knn', model_2), ('dt', model_3)], voting='hard')\n",
        " \n",
        "# Predicting the output on the test dataset\n",
        "final_model.fit(X_train, y_train)\n",
        "\n",
        "y_hat = clf.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "print(f'Accuracy Score is {accuracy_score(y_test, y_hat)}')\n",
        "\n",
        "# Model Attributes\n",
        "print(final_model.estimators_)\n",
        "print(final_model.named_estimators)\n",
        "print(final_model.le_)\n",
        "print(final_model.voting)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GxByrZHdq7r"
      },
      "source": [
        "## Averaging\n",
        "\n",
        "### Conceptual Explanation \n",
        "\n",
        "It is mainly used for regression problems. The method consists of building multiple models independently and returning the average of the prediction of all the models. In general, the combined output is better than an individual output because variance is reduced.\n",
        "\n",
        "\n",
        "**References**: \n",
        "\n",
        "- https://www.geeksforgeeks.org/ensemble-methods-in-python/\n",
        "- https://machinelearningmastery.com/ensemble-machine-learning-algorithms-python-scikit-learn/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "cVmfnmTxdq7r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "295933bb-fad3-4582-e9f1-5ff11e80714f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22.88822585335909\n",
            "18.506015686274512\n",
            "12.441143303921564\n",
            "14.35600116093697\n"
          ]
        }
      ],
      "source": [
        "# Example of averaging\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Splitting between train data into training and validation dataset\n",
        "AV_train, AV_test, av_train, av_test = train_test_split(x, y, test_size=0.20)\n",
        "\n",
        "# Initializing all the model objects with default parameters\n",
        "model_1 = LinearRegression()\n",
        "model_2 = KNeighborsRegressor()\n",
        "model_3 = RandomForestRegressor()\n",
        " \n",
        "# Training all the model on the training dataset\n",
        "model_1.fit(AV_train, av_train)\n",
        "model_2.fit(AV_train, av_train)\n",
        "model_3.fit(AV_train, av_train)\n",
        "\n",
        "# Predicting the output on the validation dataset\n",
        "pred_1 = model_1.predict(AV_test)\n",
        "pred_2 = model_2.predict(AV_test)\n",
        "pred_3 = model_3.predict(AV_test)\n",
        "\n",
        "# Final prediction after averaging on the prediction of all 3 models\n",
        "pred_final = (pred_1+pred_2+pred_3)/3.0\n",
        "\n",
        "# Printing the root mean squared error between real value and predicted value\n",
        "print(mean_squared_error(av_test, pred_1))\n",
        "print(mean_squared_error(av_test, pred_2))\n",
        "print(mean_squared_error(av_test, pred_3))\n",
        "print(mean_squared_error(av_test, pred_final))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPxkwRGcdq7s"
      },
      "source": [
        "## Weighted Average\n",
        "\n",
        "### Conceptual Explanation \n",
        "\n",
        "Weighted average or weighted sum ensemble is an ensemble machine learning approach that combines the predictions from multiple models, where the contribution of each model is weighted proportionally to its capability or skill.\n",
        "\n",
        "The weighted average ensemble is related to the voting ensemble.\n",
        "\n",
        "Voting ensembles are composed of multiple machine learning models where the predictions from each model are averaged directly. A limitation of the voting ensemble technique is that it assumes that all models in the ensemble are equally effective. This may not be the case as some models may be better than others, especially if different machine learning algorithms are used to train each model ensemble member.\n",
        "\n",
        "An alternative to voting is to assume that ensemble members are not all equally capable and instead some models are better than others and should be given more votes or more of a seat when making a prediction. This provides the motivation for the weighted sum or weighted average ensemble method.\n",
        "\n",
        "**References**: \n",
        "\n",
        "- https://machinelearningmastery.com/weighted-average-ensemble-with-python/\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingRegressor.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "GKYl1Wt0dq7s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efe27865-0839-452c-bcbd-df86b3f7718a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score is 0.9635036496350365\n",
            "[LogisticRegression(), KNeighborsClassifier(), DecisionTreeClassifier()]\n",
            "{'lr': LogisticRegression(), 'knn': KNeighborsClassifier(), 'dt': DecisionTreeClassifier()}\n",
            "LabelEncoder()\n",
            "hard\n"
          ]
        }
      ],
      "source": [
        "# Example of weighted averaging\n",
        "\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Initializing all the model objects with default parameters\n",
        "model_1 = LogisticRegression()\n",
        "model_2 = KNeighborsClassifier()\n",
        "model_3 = DecisionTreeClassifier()\n",
        "\n",
        "# Weights for the models\n",
        "weights = [0.5, 0.7, 0.9]\n",
        "\n",
        "# Making the final model using voting classifier\n",
        "final_model = VotingClassifier(estimators=[('lr', model_1), ('knn', model_2), ('dt', model_3)], weights=weights)\n",
        " \n",
        "# Predicting the output on the test dataset\n",
        "final_model.fit(X_train, y_train)\n",
        "\n",
        "y_hat = clf.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "print(f'Accuracy Score is {accuracy_score(y_test, y_hat)}')\n",
        "\n",
        "# Model Attributes\n",
        "print(final_model.estimators_)\n",
        "print(final_model.named_estimators)\n",
        "print(final_model.le_)\n",
        "print(final_model.voting)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o91kZWLNdq7s"
      },
      "source": [
        "# Advanced Ensemble Methods\n",
        "\n",
        "## Bagging\n",
        "\n",
        "### Conceptual Explanation \n",
        "\n",
        "It is also known as a bootstrapping method. Base models are run on bags to get a fair distribution of the whole dataset. A bag is a subset of the dataset along with a replacement to make the size of the bag the same as the whole dataset. The final output is formed after combining the output of all base models. \n",
        "\n",
        "Algorithm:\n",
        "\n",
        "1. Create multiple datasets from the train dataset by selecting observations with replacements.\n",
        "2. Run a base model on each of the created datasets independently.\n",
        "3. Combine the predictions of all the base models to each the final output.\n",
        "\n",
        "**References**: \n",
        "\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html#sklearn.ensemble.BaggingRegressor\n",
        "- https://www.geeksforgeeks.org/ensemble-methods-in-python/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "bVe3R4Gcdq7s",
        "outputId": "4897a551-8271-4fc9-da1f-625ca5e0ea9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score is 0.9708029197080292\n"
          ]
        }
      ],
      "source": [
        "# Example of Bagging Classifier \n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "\n",
        "clf = BaggingClassifier(base_estimator=SVC(), n_estimators=10, random_state=0)\n",
        "clf.fit(X_train, y_train)\n",
        "y_hat = clf.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "print(f'Accuracy Score is {accuracy_score(y_test, y_hat)}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-zWsx4Odq7s"
      },
      "source": [
        "## Boosting\n",
        "\n",
        "### Conceptual Explanation \n",
        "\n",
        "Boosting is a sequential method–it aims to prevent a wrong base model from affecting the final output. Instead of combing the base models, the method focuses on building a new model that is dependent on the previous one. A new model tries to remove the errors made by its previous one. Each of these models is called weak learners. The final model (aka strong learner) is formed by getting the weighted mean of all the weak learners. \n",
        "\n",
        "Algorithm:\n",
        "\n",
        "1. Take a subset of the train dataset.\n",
        "2. Train a base model on that dataset.\n",
        "3. Use third model to make predictions on the whole dataset.\n",
        "4. Calculate errors using the predicted values and actual values.\n",
        "5. Initialize all data points with same weight.\n",
        "6. Assign higher weight to incorrectly predicted data points.\n",
        "7. Make another model, make predictions using the new model in such a way that errors made by the previous model are mitigated/corrected.\n",
        "8. Similarly, create multiple models–each successive model correcting the errors of the previous model.\n",
        "9. The final model (strong learner) is the weighted mean of all the previous models (weak learners).\n",
        "\n",
        "**References**: \n",
        "\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "73y9PJQFdq7t",
        "outputId": "244703b4-633a-47b5-f0f7-fdbc5b87ea9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score is 0.9708029197080292\n"
          ]
        }
      ],
      "source": [
        "# Example of Bosting Classifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n",
        "clf.fit(X_train, y_train)\n",
        "y_hat = clf.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "print(f'Accuracy Score is {accuracy_score(y_test, y_hat)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zb8WNtRdq7t"
      },
      "source": [
        "## Stacking\n",
        "\n",
        "### Conceptual Explanation \n",
        "\n",
        "It is an ensemble method that combines multiple models (classification or regression) via meta-model (meta-classifier or meta-regression). The base models are trained on the complete dataset, then the meta-model is trained on features returned (as output) from base models. The base models in stacking are typically different. The meta-model helps to find the features from base models to achieve the best accuracy.\n",
        "\n",
        "Algorithm:\n",
        "\n",
        "1. Split the train dataset into n parts\n",
        "2. A base model (say linear regression) is fitted on n-1 parts and predictions are made for the nth part. This is done for each one of the n part of the train set.\n",
        "3. The base model is then fitted on the whole train dataset.\n",
        "4. This model is used to predict the test dataset.\n",
        "5. The Steps 2 to 4 are repeated for another base model which results in another set of predictions for the train and test dataset.\n",
        "6. The predictions on train data set are used as a feature to build the new model.\n",
        "7. This final model is used to make the predictions on test dataset\n",
        "\n",
        "**References**: \n",
        "\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "ArxmE1Gydq7t",
        "outputId": "af2f5022-3793-4b7f-d466-78cc0bd6de02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score is 0.9562043795620438\n"
          ]
        }
      ],
      "source": [
        "# Example of Stacking Classifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "estimators = [\n",
        "    ('rf', RandomForestClassifier(n_estimators=10, random_state=42)),\n",
        "    ('svr', make_pipeline(StandardScaler(), LinearSVC(random_state=42)))\n",
        "]\n",
        "\n",
        "clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n",
        "clf.fit(X_train, y_train)\n",
        "y_hat = clf.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "print(f'Accuracy Score is {accuracy_score(y_test, y_hat)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyW9o5-tdq7t"
      },
      "source": [
        "## Blending\n",
        "\n",
        "### Conceptual Explanation \n",
        "\n",
        "It is similar to the stacking method explained above, but rather than using the whole dataset for training the base-models, a validation dataset is kept separate to make predictions. \n",
        "\n",
        "1. Split the training dataset into train, test and validation dataset.\n",
        "2. Fit all the base models using train dataset.\n",
        "3. Make predictions on validation and test dataset.\n",
        "4. These predictions are used as features to build a second level model\n",
        "5. This model is used to make predictions on test and meta-features\n",
        "\n",
        "**References**: \n",
        "\n",
        "- https://www.geeksforgeeks.org/ensemble-methods-in-python/ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "LCJGCJ7ddq7t",
        "outputId": "15354ed5-bcff-4d0e-fa0e-a8df7c9a67d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16.97274343223606\n"
          ]
        }
      ],
      "source": [
        "# Example of blending\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Performing the train test and validation split\n",
        "train_ratio = 0.70\n",
        "validation_ratio = 0.20\n",
        "test_ratio = 0.10\n",
        "\n",
        "# Splitting between train data into training and validation dataset\n",
        "B_train, B_test, b_train, b_test = train_test_split(x, y, test_size=1-train_ratio)\n",
        "\n",
        "# Performing test validation split\n",
        "B_val, B_test, b_val, b_test = train_test_split(B_test, b_test, test_size=test_ratio/(test_ratio + validation_ratio))\n",
        "\n",
        "# Initializing all the model objects with default parameters\n",
        "model_1 = LinearRegression()\n",
        "model_2 = KNeighborsRegressor()\n",
        "model_3 = RandomForestRegressor()\n",
        "\n",
        "# Training all the model on the train dataset\n",
        " \n",
        "# Training first model\n",
        "model_1.fit(B_train, b_train)\n",
        "val_pred_1 = model_1.predict(B_val)\n",
        "test_pred_1 = model_1.predict(B_test)\n",
        " \n",
        "# Converting to dataframe\n",
        "val_pred_1 = pd.DataFrame(val_pred_1)\n",
        "test_pred_1 = pd.DataFrame(test_pred_1)\n",
        " \n",
        "# Training second model\n",
        "model_2.fit(B_train, b_train)\n",
        "val_pred_2 = model_2.predict(B_val)\n",
        "test_pred_2 = model_2.predict(B_test)\n",
        " \n",
        "# Converting to dataframe\n",
        "val_pred_2 = pd.DataFrame(val_pred_2)\n",
        "test_pred_2 = pd.DataFrame(test_pred_2)\n",
        " \n",
        "# Training third model\n",
        "model_3.fit(B_train, b_train)\n",
        "val_pred_3 = model_1.predict(B_val)\n",
        "test_pred_3 = model_1.predict(B_test)\n",
        " \n",
        "# Converting to dataframe\n",
        "val_pred_3 = pd.DataFrame(val_pred_3)\n",
        "test_pred_3 = pd.DataFrame(test_pred_3)\n",
        " \n",
        "# Concatenating validation dataset along with all the predicted validation data (meta features)\n",
        "df_val = pd.concat([B_val, val_pred_1, val_pred_2, val_pred_3], axis=1)\n",
        "df_test = pd.concat([B_test, test_pred_1, test_pred_2, test_pred_3], axis=1)\n",
        " \n",
        "# Making the final model using the meta features\n",
        "final_model = LinearRegression()\n",
        "final_model.fit(B_train, b_train)\n",
        " \n",
        "# Getting the final output\n",
        "final_pred = final_model.predict(B_test)\n",
        " \n",
        "#printing the root mean squared error between real value and predicted value\n",
        "print(mean_squared_error(b_test, final_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SxcAjeNdq7u"
      },
      "source": [
        "# Is it possible to create our own ensemble model? If so, how would it be the approach?\n",
        "\n",
        "Yes, it is possible to create our own ensemble model. The easiest way will be to create series of models and then combine them using the Simpple Ensemble Methods as Max Voting, Averaging, Weighted Average, Bagging, Boosting, Stacking, or Blending.\n",
        "\n",
        "**References**: \n",
        "\n",
        "- https://towardsdatascience.com/ensemble-learning-using-scikit-learn-85c4531ff86a\n",
        "- https://scikit-learn.org/stable/modules/ensemble.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9DQSzajdq7u"
      },
      "source": [
        "# Metrics\n",
        "\n",
        "Performance metrics are a part of every machine learning pipeline. They tell you if you’re making progress, and put a number on it. All machine learning models, whether it’s linear regression, or a SOTA technique like BERT, need a metric to judge performance\n",
        "\n",
        "**References**: \n",
        "\n",
        "- https://neptune.ai/blog/performance-metrics-in-machine-learning-complete-guide\n",
        "- https://scikit-learn.org/stable/modules/model_evaluation.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_4zmUIodq7u"
      },
      "source": [
        "## Classification Metrics\n",
        "\n",
        "Classification models have discrete output, so we need a metric that compares discrete classes in some form. Classification Metrics evaluate a model’s performance and tell you how good or bad the classification is, but each of them evaluates it in a different way.\n",
        "\n",
        "The following are the metrics for evaluating classification problems:\n",
        "\n",
        "- Accuracy\n",
        "- Balanced Accuracy\n",
        "- Top k Accuracy\n",
        "- Average Precision\n",
        "- Brier Score\n",
        "- F1 Score\n",
        "- F1 Micro Score\n",
        "- F1 Macro Score\n",
        "- F1 Weighted Score\n",
        "- F1 Sample Score\n",
        "- Log Loss\n",
        "- Precision\n",
        "- Recall\n",
        "- Jaccard\n",
        "- Roc Auc\n",
        "- Roc Auc Overlap\n",
        "- Roc Auc Under Over\n",
        "\n",
        "In the following seccion, we will discuss more details of the most common metrics.\n",
        "\n",
        "### Confusion Matrix\n",
        "\n",
        "Confusion Matrix is a tabular visualization of the ground-truth labels versus model predictions. Each row of the confusion matrix represents the instances in a predicted class and each column represents the instances in an actual class. Confusion Matrix is not exactly a performance metric but sort of a basis on which other metrics evaluate the results. One example of a confusion matrix is the following image:\n",
        "\n",
        "![confusion](https://drive.google.com/uc?id=11WoKsUWrbRHMaQ1GvlYBzgwV_YLb7-NP)\n",
        "\n",
        "The confusion matrix is made up of the following elements:\n",
        "\n",
        "- **True Positives**: Number of correct predictions of the positive class.\n",
        "- **True Negatives**: Number of correct predictions of the negative class.\n",
        "- **False Positives**: Number of incorrect predictions of the positive class.\n",
        "- **False Negatives**: Number of incorrect predictions of the negative class.\n",
        "\n",
        "The idea is to try minimize the **False Positives** and **False Negatives**, since these are the ones that are most likely to be misclassified.\n",
        "\n",
        "### Accuracy\n",
        "\n",
        "Classification accuracy is perhaps the simplest metric to use and implement and is defined as the number of correct predictions divided by the total number of predictions, multiplied by 100.\n",
        "\n",
        "This metric is described with the following equation:\n",
        "\n",
        "\\begin{equation}\n",
        "Accuracy = \\frac{TP + TN}{TP + FP + TN + FN}\n",
        "\\end{equation}\n",
        "\n",
        "### Precision\n",
        "\n",
        "There are many cases in which classification accuracy is not a good indicator of your model performance. One of these scenarios is when your class distribution is imbalanced (one class is more frequent than others). In this case, even if you predict all samples as the most frequent class you would get a high accuracy rate, which does not make sense at all (because your model is not learning anything, and is just predicting everything as the top class). Precision is the ratio of true positives and total positives predicted. \n",
        "\n",
        "This metric is described with the following equation:\n",
        "\n",
        "\\begin{equation}\n",
        "Precision = \\frac{TP}{TP+FP}\n",
        "\\end{equation}\n",
        "\n",
        "### Recall\n",
        "\n",
        "Recall is another important metric, which is defined as the fraction of samples from a class which are correctly predicted by the model.\n",
        "\n",
        "This metric is described with the following equation:\n",
        "\n",
        "\\begin{equation}\n",
        "Recall = \\frac{TP}{TP+FN}\n",
        "\\end{equation}\n",
        "\n",
        "### F1\n",
        "\n",
        "Depending on application, you may want to give higher priority to recall or precision. But there are many applications in which both recall and precision are important. Therefore, it is natural to think of a way to combine these two into a single metric. One popular metric which combines precision and recall is called F1-score, which is the harmonic mean of precision and recall.\n",
        "\n",
        "This metric is described with the following equation:\n",
        "\n",
        "\\begin{equation}\n",
        "F1 = \\frac{1}{\\frac{1}{Precision} + \\frac{1}{Recall}}\n",
        "\\end{equation}\n",
        "\n",
        "### Receiver Operating Curve (ROC) \n",
        "\n",
        "The receiver operating characteristic curve is plot which shows the performance of a binary classifier as function of its cut-off threshold. It essentially shows the true positive rate (TPR) against the false positive rate (FPR) for various threshold values.\n",
        "\n",
        "One example of a ROC curve is the following image:\n",
        "\n",
        "![roc](https://drive.google.com/uc?id=11bIlNV5ZT67ykPlTeiQD6Qd_yf7XCeKK)\n",
        "\n",
        "### Area Under the Curve (AUC)\n",
        "\n",
        "The area under the curve (AUC), is an aggregated measure of performance of a binary classifier on all possible threshold values (and therefore it is threshold invariant). AUC calculates the area under the ROC curve, and therefore it is between 0 and 1. One way of interpreting AUC is as the probability that the model ranks a random positive example more highly than a random negative example.\n",
        "\n",
        "One example of a AUC curve is the following image:\n",
        "\n",
        "![auc](https://drive.google.com/uc?id=11eAaRyfUK13HxSM9gkbp9yHPeK1AlcXL)\n",
        "\n",
        "**References**: \n",
        "\n",
        "- https://neptune.ai/blog/performance-metrics-in-machine-learning-complete-guide#:~:text=Classification%20models%20have%20discrete%20output,it%20in%20a%20different%20way.\n",
        "- https://medium.com/@MohammedS/performance-metrics-for-classification-problems-in-machine-learning-part-i-b085d432082b\n",
        "- https://towardsdatascience.com/20-popular-machine-learning-metrics-part-1-classification-regression-evaluation-metrics-1ca3e282a2ce"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "B-_XSVWadq7u"
      },
      "outputs": [],
      "source": [
        "# Simple Example of Classification Metrics\n",
        "# Notebook from: https://app.neptune.ai/theaayushbajaj/sandbox/n/f884bbea-5263-4aeb-aa35-18d74b2835b9/41813125-2b9d-4332-b73f-f07c3b977372\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Train a model\n",
        "clf = SVC(\n",
        "        C=1.0,\n",
        "        kernel='rbf',\n",
        "        degree=3,\n",
        "        gamma='scale',\n",
        "        coef0=0.0,\n",
        "        shrinking=True,\n",
        "        probability=False,\n",
        "        tol=0.001,\n",
        "        cache_size=200,\n",
        "        class_weight=None,\n",
        "        verbose=False,\n",
        "        max_iter=-1,\n",
        "        decision_function_shape='ovr',\n",
        "        break_ties=False,\n",
        "        random_state=None\n",
        "    )\n",
        "\n",
        "clf.fit(X_train,y_train)\n",
        "\n",
        "y_hat = clf.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "sizFP7gpdq7v",
        "outputId": "77e0fb8a-af46-486e-a422-0c224a2c733d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[85  2]\n",
            " [ 3 47]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_hat)\n",
        "print('Confusion Matrix:')\n",
        "print(cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "GTEK0qlxdq7v",
        "outputId": "2bbb8b3a-10c8-4de5-eb64-dee42de7a708",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score is 0.9635036496350365\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Accuracy\n",
        "print(f'Accuracy Score is {accuracy_score(y_test,y_hat)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "BHIjLi3fdq7v",
        "outputId": "306db79d-7ae2-4632-826d-ab8a8865ec87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.959184\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import precision_score\n",
        "# Precision\n",
        "precision = precision_score(y_test, y_hat)\n",
        "print('Precision: %f' % precision)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "19Px7R0Sdq7w",
        "outputId": "05d01b02-89cf-4e4e-9fdb-0b5d930ba9eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recall: 0.940000\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import recall_score\n",
        "# Recall\n",
        "recall = recall_score(y_test, y_hat)\n",
        "print('Recall: %f' % recall)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "T7DsInWEdq7w",
        "outputId": "0fc6808f-6eb4-4dfc-af47-1e3037795d9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score: 0.949495\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "# F1 Score\n",
        "f1 = f1_score(y_test, y_hat)\n",
        "print('F1 Score: %f' % f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "K7dmbMR3dq7w",
        "outputId": "432bc8a3-df4a-4208-c1a4-329c019259c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, '2-class Precision-Recall curve: AP=0.96')"
            ]
          },
          "metadata": {},
          "execution_count": 36
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "from sklearn.metrics import plot_precision_recall_curve\n",
        "\n",
        "# Plot Precision-Recall Curve\n",
        "disp = plot_precision_recall_curve(clf, X, Y)\n",
        "disp.ax_.set_title('2-class Precision-Recall curve: AP={0:0.2f}'.format(precision))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "U4fBmjA9dq7w",
        "outputId": "fdc539a6-c183-40e1-a234-fb7cfccdf529",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No Skill: ROC AUC=0.500\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "ns_probs = [0 for _ in range(len(Y))]\n",
        "\n",
        "# calculate scores\n",
        "ns_auc = roc_auc_score(Y, ns_probs)\n",
        "\n",
        "# summarize scores\n",
        "print('No Skill: ROC AUC=%.3f' % (ns_auc))\n",
        "\n",
        "# calculate roc curves\n",
        "ns_fpr, ns_tpr, _ = roc_curve(Y, ns_probs)\n",
        "\n",
        "# plot the roc curve for the model\n",
        "pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
        "\n",
        "pyplot.xlabel('False Positive Rate')\n",
        "pyplot.ylabel('True Positive Rate')\n",
        "\n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdH5Jqc6dq7x"
      },
      "source": [
        "## Regression Metrics\n",
        "\n",
        "Regression refers to predictive modeling problems that involve predicting a numeric value.\n",
        "\n",
        "It is different from classification that involves predicting a class label. Unlike classification, you cannot use classification accuracy to evaluate the predictions made by a regression model. Instead, you must use error metrics specifically designed for evaluating predictions made on regression problems and it is most Robust to outliers.\n",
        "\n",
        "The following are the metrics for evaluating regression problems:\n",
        "\n",
        "- Max Error\n",
        "- Mean Absolute Error\n",
        "- Mean Squared Error\n",
        "- Root Mean Squared Error\n",
        "- Mean Squared Logarithmic Error\n",
        "- Median Absolute Error\n",
        "- R-Squared\n",
        "- Explained Variance\n",
        "- Mean Poisson Deviance\n",
        "- Mean Gamma Deviance\n",
        "- Mean Absolute Percentage Error\n",
        "\n",
        "In the following seccion, we will discuss more details of the most common metrics.\n",
        "\n",
        "### Mean Absolute Error (MAE)\n",
        "\n",
        "MAE is a very simple metric which calculates the absolute difference between actual and predicted values. It has the advantage that the MAE you get is in the same unit as the output variable.\n",
        "\n",
        "In contrast, the disadvantage is that the graph of MAE is not differentiable so we have to apply various optimizers like gradient descent which can be differentiable.\n",
        "\n",
        "This metric is described with the following equation:\n",
        "\n",
        "\\begin{equation}\n",
        "MAE = \\frac{1}{n} \\sum_{1=n}^{i=1} \\left|h(x_n) - y_n\\right|\n",
        "\\end{equation}\n",
        "\n",
        "### Mean Squared Error (MSE)\n",
        "\n",
        "It represents the squared distance between actual and predicted values. we perform squared to avoid the cancellation of negative terms and it is the benefit of MSE. MSE is a most used and very simple metric with a little bit of change in mean absolute error. Mean squared error states that finding the squared difference between actual and predicted value.\n",
        "\n",
        "So, above we are finding the absolute difference and here we are finding the squared difference. The graph of MSE is differentiable, so you can easily use it as a loss function.\n",
        "\n",
        "It has the disadvantage that the value you get after calculating MSE is a squared unit of output. For example, the output variable is in meter (m) then after calculating MSE the output we get is in meter squared and if the data has outliers t then it penalizes the outliers most and the calculated MSE is bigger. So, in short, it is not robust to outliers which were an advantage in MAE.\n",
        "\n",
        "This metric is described with the following equation:\n",
        "\n",
        "\\begin{equation}\n",
        "MSE = \\frac{1}{n} \\sum_{1=n}^{i=1} \\left(h(x_n) - y_n\\right)^2\n",
        "\\end{equation}\n",
        "\n",
        "### Root Mean Squared Error (RMSE)\n",
        "\n",
        "As RMSE is clear by the name itself, that it is a simple square root of mean squared error. It has the advantage that the output value you get is in the same unit as the required output variable which makes interpretation of loss easy, but it is not that robust to outliers as compared to MAE.\n",
        "\n",
        "This metric is described with the following equation:\n",
        "\n",
        "\\begin{equation}\n",
        "RMSE = \\sqrt{\\frac{1}{n} \\sum_{1=n}^{i=1} \\left(h(x_n) - y_n\\right)^2}\n",
        "\\end{equation}\n",
        "\n",
        "### Root Mean Squared Logarithmic Error (RMSLE)\n",
        "\n",
        "Taking the $\\log$ of the RMSE metric slows down the scale of error. The metric is very helpful when you are developing a model without calling the inputs. In that case, the output will vary on a large scale. To control this situation of RMSE we take the $\\log$  of calculated RMSE error and resultant we get as RMSLE.\n",
        "\n",
        "This metric is described with the following equation:\n",
        "\n",
        "\\begin{equation}\n",
        "RMSLE = \\sqrt{\\frac{1}{n} \\sum_{1=n}^{i=1} \\left( \\log \\left(h(x_n) + 1\\right) - \\log \\left( y_n + 1 \\right) \\right)^2}\n",
        "\\end{equation}\n",
        "\n",
        "### R Squared ($R^2$)\n",
        "\n",
        "$R^2$ score is a metric that tells the performance of your model, not the loss in an absolute sense that how many wells did your model perform. In contrast, MAE and MSE depend on the context as we have seen whereas the R2 score is independent of context.\n",
        "\n",
        "So, with help of R squared we have a baseline model to compare a model which none of the other metrics provides. The same we have in classification problems which we call a threshold which is fixed at 0.5. So basically R2 squared calculates how must regression line is better than a mean line.\n",
        "\n",
        "This metric is described with the following equation:\n",
        "\n",
        "\\begin{equation}\n",
        "R^2 = 1 - \\frac{RSS}{TSS}\n",
        "\\end{equation}\n",
        "\n",
        "Where RSS is the Residual Sum of Squares and TSS is the Total Sum of Squares. They are described with the following formulas:\n",
        "\n",
        "\\begin{equation}\n",
        "RSS =  \\sum_{1=n}^{i=1} \\left(h(x_n) - y_n\\right)^2\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "TSS =  \\sum_{1=n}^{i=1} \\left(h(x_n) - \\overline{y}\\right)^2\n",
        "\\end{equation}\n",
        "\n",
        "### Adjusted R Squared\n",
        "\n",
        "The disadvantage of the R2 score is while adding new features in data the R2 score starts increasing or remains constant but it never decreases because It assumes that while adding more data variance of data increases.\n",
        "\n",
        "But the problem is when we add an irrelevant feature in the dataset then at that time R2 sometimes starts increasing which is incorrect. Hence, To control this situation adjusted R Squared came into existence.\n",
        "\n",
        "This metric is described with the following equation:\n",
        "\n",
        "\\begin{equation}\n",
        "R^2_a = 1 - \\left| \\left( \\frac{n-1}{n-k-1}\\right) \\times  \\left(1 - R2\\right) \\right|\n",
        "\\end{equation}\n",
        "\n",
        "Where $k$ is the number of independent variables, $n$ is the number of observations, and $R^2$ is the R squared value.\n",
        "\n",
        "**References**: \n",
        "\n",
        "- https://towardsdatascience.com/20-popular-machine-learning-metrics-part-1-classification-regression-evaluation-metrics-1ca3e282a2ce\n",
        "- https://machinelearningmastery.com/regression-metrics-for-machine-learning/\n",
        "- https://www.analyticsvidhya.com/blog/2021/05/know-the-best-evaluation-metrics-for-your-regression-model/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "MJ9XFbUudq7x"
      },
      "outputs": [],
      "source": [
        "# Simple Example of Regression Metrics\n",
        "# Notebook from: https://app.neptune.ai/theaayushbajaj/sandbox/n/f884bbea-5263-4aeb-aa35-18d74b2835b9/41813125-2b9d-4332-b73f-f07c3b977372\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "regressor = LinearRegression()\n",
        "regressor.fit(x, y)\n",
        "y_hat = regressor.predict(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "msninhmydq7y",
        "outputId": "7efb5b79-2f15-482b-d711-5727c2e25212",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE: 3.27 (+/- 3.35)\n"
          ]
        }
      ],
      "source": [
        "mae = np.abs(y-y_hat)\n",
        "\n",
        "print(f\"MAE: {mae.mean():0.2f} (+/- {mae.std():0.2f})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "BxzdOUzBdq7y",
        "outputId": "f9ce67eb-228a-4c03-b31e-430651c40131",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 21.89 (+/- 59.14)\n"
          ]
        }
      ],
      "source": [
        "mse = (y-y_hat)**2\n",
        "\n",
        "print(f\"MSE: {mse.mean():0.2f} (+/- {mse.std():0.2f})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "nDcAHlSGdq7z",
        "outputId": "7e79b2a2-0622-4dde-a786-d58efe1358c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE: 4.68\n"
          ]
        }
      ],
      "source": [
        "mse = (y-y_hat)**2\n",
        "\n",
        "rmse = np.sqrt(mse.mean())\n",
        "\n",
        "print(f\"RMSE: {rmse:0.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "LS3fJilGdq7z",
        "outputId": "c67c151b-8b3a-4e07-b208-5e3a7d6d0e2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R^2 coefficient of determination: 74.06%\n"
          ]
        }
      ],
      "source": [
        "# R^2 coefficient of determination\n",
        "SE_line = sum((y-y_hat)**2)\n",
        "SE_mean = sum((y-y.mean())**2)\n",
        "\n",
        "r2 = 1-(SE_line/SE_mean)\n",
        "\n",
        "print(f\"R^2 coefficient of determination: {r2*100:0.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Notebook2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "BGXa44cgdq7f",
        "7Ia33g54dq7h",
        "6vlx0AFCdq7l",
        "3ndVrgh1dq7m",
        "VDtAhNAxdq7n",
        "pbCTMfYWdq7o",
        "gGbGmf1Mdq7p",
        "JJBrCZ4Sdq7q",
        "2GxByrZHdq7r",
        "XPxkwRGcdq7s",
        "p-zWsx4Odq7s",
        "4zb8WNtRdq7t",
        "LyW9o5-tdq7t",
        "J_4zmUIodq7u",
        "DdH5Jqc6dq7x"
      ]
    },
    "interpreter": {
      "hash": "762531c616344ec89a3ae3efe707c5228b11104740e698ce334efdf481e7418a"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}